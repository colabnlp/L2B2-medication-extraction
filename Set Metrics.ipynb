{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import Functions as fn\n",
    "from DS import DS\n",
    "from Set import pool\n",
    "from Iterator import Iterator\n",
    "from FFModel import FF_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Load Complete\n",
      "Raw Labels Load Complete\n"
     ]
    }
   ],
   "source": [
    "Dataset = pool()\n",
    "Dataset.load_texts('raw_texts')\n",
    "Dataset.load_labels('raw_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585 238 10 10\n"
     ]
    }
   ],
   "source": [
    "train_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[:-10])\n",
    "validation_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[-10:])\n",
    "test_set = Dataset.get_DS(stage='train', labelled='yes')\n",
    "set_1 = Dataset.get_DS(stage='train', labelled='no')\n",
    "set_2 = Dataset.get_DS(stage='test', labelled='no')\n",
    "set_1.append(set_2.data)\n",
    "set_1.append(train_set.data)\n",
    "emb_set = set_1\n",
    "print(emb_set.size, train_set.size, validation_set.size, test_set.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dataset.process_for_embedding()\n",
    "emb_set.process_for_embedding()\n",
    "train_set.process_for_embedding()\n",
    "validation_set.process_for_embedding()\n",
    "test_set.process_for_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in All Sets: 246168\n",
      "Word Tokens in All Sets: 4461658\n",
      "Avg Sent/EHR in All Sets: 53.45667752442997\n",
      "Avg WT/EHR in All Sets: 968.872529858849\n",
      "Avg WT/sent in All Sets: 18.124443469500505\n"
     ]
    }
   ],
   "source": [
    "sn = 0\n",
    "wn = 0\n",
    "for case in Dataset.data:\n",
    "    sn += len(case.emb_text)\n",
    "    for sent in case.emb_text:\n",
    "        wn += len(sent)\n",
    "print('Sentences in All Sets: {}'.format(sn))\n",
    "print('Word Tokens in All Sets: {}'.format(wn))\n",
    "print('Avg Sent/EHR in All Sets: {}'.format(sn/Dataset.size))\n",
    "print('Avg WT/EHR in All Sets: {}'.format(wn/Dataset.size))\n",
    "print('Avg WT/sent in All Sets: {}'.format(wn/sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in Emb Set: 244979\n",
      "Word Tokens in Emb Set: 4437268\n",
      "Avg Sent/EHR in Emb Set: 53.43053435114504\n",
      "Avg WT/EHR in Emb Set: 967.779280261723\n",
      "Avg WT/sent in Emb Set: 18.112850489225607\n"
     ]
    }
   ],
   "source": [
    "sn = 0\n",
    "wn = 0\n",
    "for case in emb_set.data:\n",
    "    sn += len(case.emb_text)\n",
    "    for sent in case.emb_text:\n",
    "        wn += len(sent)\n",
    "print('Sentences in Emb Set: {}'.format(sn))\n",
    "print('Word Tokens in Emb Set: {}'.format(wn))\n",
    "print('Avg Sent/EHR in Emb Set: {}'.format(sn/emb_set.size))\n",
    "print('Avg WT/EHR in Emb Set: {}'.format(wn/emb_set.size))\n",
    "print('Avg WT/sent in Emb Set: {}'.format(wn/sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in Train Sets: 14050\n",
      "Word Tokens in Train Set: 268272\n",
      "Avg Sent/EHR in Train Set: 59.03361344537815\n",
      "Avg WT/EHR in Train Set: 1127.1932773109243\n",
      "Avg WT/sent in Train Set: 19.094092526690392\n"
     ]
    }
   ],
   "source": [
    "sn = 0\n",
    "wn = 0\n",
    "for case in train_set.data:\n",
    "    sn += len(case.emb_text)\n",
    "    for sent in case.emb_text:\n",
    "        wn += len(sent)\n",
    "print('Sentences in Train Sets: {}'.format(sn))\n",
    "print('Word Tokens in Train Set: {}'.format(wn))\n",
    "print('Avg Sent/EHR in Train Set: {}'.format(sn/train_set.size))\n",
    "print('Avg WT/EHR in Train Set: {}'.format(wn/train_set.size))\n",
    "print('Avg WT/sent in Train Set: {}'.format(wn/sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in Val Sets: 721\n",
      "Word Tokens in Val Sets: 13230\n",
      "Avg Sent/EHR in Val Sets: 72.1\n",
      "Avg WT/EHR in Val Set: 1323.0\n",
      "Avg WT/sent in Val Sets: 18.349514563106798\n"
     ]
    }
   ],
   "source": [
    "sn = 0\n",
    "wn = 0\n",
    "for case in validation_set.data:\n",
    "    sn += len(case.emb_text)\n",
    "    for sent in case.emb_text:\n",
    "        wn += len(sent)\n",
    "print('Sentences in Val Sets: {}'.format(sn))\n",
    "print('Word Tokens in Val Sets: {}'.format(wn))\n",
    "print('Avg Sent/EHR in Val Sets: {}'.format(sn/validation_set.size))\n",
    "print('Avg WT/EHR in Val Set: {}'.format(wn/validation_set.size))\n",
    "print('Avg WT/sent in Val Sets: {}'.format(wn/sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in Test Sets: 468\n",
      "Word Tokens in Test Sets: 11160\n",
      "Avg Sent/EHR in Test Sets: 46.8\n",
      "Avg WT/EHR in Test Set: 1116.0\n",
      "Avg WT/sent in Test Sets: 23.846153846153847\n"
     ]
    }
   ],
   "source": [
    "sn = 0\n",
    "wn = 0\n",
    "for case in test_set.data:\n",
    "    sn += len(case.emb_text)\n",
    "    for sent in case.emb_text:\n",
    "        wn += len(sent)\n",
    "print('Sentences in Test Sets: {}'.format(sn))\n",
    "print('Word Tokens in Test Sets: {}'.format(wn))\n",
    "print('Avg Sent/EHR in Test Sets: {}'.format(sn/test_set.size))\n",
    "print('Avg WT/EHR in Test Set: {}'.format(wn/test_set.size))\n",
    "print('Avg WT/sent in Test Sets: {}'.format(wn/sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Dataset.get_sentences()\n",
    "model = Word2Vec(sentences, min_count=1, size=10)\n",
    "all_vocab = model.wv.vocab.keys()\n",
    "\n",
    "sentences = emb_set.get_sentences()\n",
    "model = Word2Vec(sentences, min_count=1, size=10)\n",
    "emb_vocab = model.wv.vocab.keys()\n",
    "\n",
    "sentences = train_set.get_sentences()\n",
    "model = Word2Vec(sentences, min_count=1, size=10)\n",
    "train_vocab = model.wv.vocab.keys()\n",
    "\n",
    "sentences = validation_set.get_sentences()\n",
    "model = Word2Vec(sentences, min_count=1, size=10)\n",
    "val_vocab = model.wv.vocab.keys()\n",
    "\n",
    "sentences = test_set.get_sentences()\n",
    "model = Word2Vec(sentences, min_count=1, size=10)\n",
    "test_vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Vocab Size: 50123\n",
      "Emb Vocab Size: 50111\n",
      "Train Vocab Size: 14529\n",
      "Val Vocab Size: 2480\n",
      "Test Vocab Size: 2287\n"
     ]
    }
   ],
   "source": [
    "print('All Vocab Size: {}'.format(len(all_vocab)))\n",
    "print('Emb Vocab Size: {}'.format(len(emb_vocab)))\n",
    "print('Train Vocab Size: {}'.format(len(train_vocab)))\n",
    "print('Val Vocab Size: {}'.format(len(val_vocab)))\n",
    "print('Test Vocab Size: {}'.format(len(test_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Val Words not in Emb Vocab: 5\n",
      "Number of Test Words not in Emb Vocab: 7\n",
      "Number of All Words not in Emb Vocab: 12\n",
      "Number of Emb Words not in Train Vocab: 35582\n",
      "Number of Val Words not in Train Vocab: 373\n",
      "Number of Test Words not in Train Vocab: 324\n",
      "Number of All Words not in Train Vocab: 35594\n"
     ]
    }
   ],
   "source": [
    "print('Number of Val Words not in Emb Vocab: {}'.format(len([word for word in val_vocab if word not in emb_vocab])))\n",
    "print('Number of Test Words not in Emb Vocab: {}'.format(len([word for word in test_vocab if word not in emb_vocab])))\n",
    "print('Number of All Words not in Emb Vocab: {}'.format(len([word for word in all_vocab if word not in emb_vocab])))\n",
    "print('Number of Emb Words not in Train Vocab: {}'.format(len([word for word in emb_vocab if word not in train_vocab])))\n",
    "print('Number of Val Words not in Train Vocab: {}'.format(len([word for word in val_vocab if word not in train_vocab])))\n",
    "print('Number of Test Words not in Train Vocab: {}'.format(len([word for word in test_vocab if word not in train_vocab])))\n",
    "print('Number of All Words not in Train Vocab: {}'.format(len([word for word in all_vocab if word not in train_vocab])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wn<num>/<num>\n",
      "h<num>su<num>m<num>ce\n",
      "tn&ltassay\n",
      "d<num>gqoz<num>v<num>\n",
      "fhow<num>s<num>\n"
     ]
    }
   ],
   "source": [
    "for word in val_vocab:\n",
    "    if word not in emb_vocab:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fhow<num>s<num>', 'd', '<num>/<num>', 't', '<num>/<num>', '[report_end]']\n"
     ]
    }
   ],
   "source": [
    "for case in validation_set.data:\n",
    "    for sent in case.emb_text:\n",
    "        if 'fhow<num>s<num>' in sent:\n",
    "            print(sent)\n",
    "            #print(case.raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
