{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import Functions as fn\n",
    "import Iterator as it\n",
    "from DS import DS\n",
    "from Set import pool\n",
    "from FFModel import FF_Model\n",
    "from RNNModel import RNN_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Load Complete\n",
      "Raw Labels Load Complete\n"
     ]
    }
   ],
   "source": [
    "Dataset = pool()\n",
    "Dataset.load_texts('raw_texts')\n",
    "Dataset.load_labels('raw_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Load Complete\n"
     ]
    }
   ],
   "source": [
    "target_dict = fn.load_labels('final_meta/labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585 238 10 10\n"
     ]
    }
   ],
   "source": [
    "train_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[:-10])\n",
    "validation_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[-10:])\n",
    "test_set = Dataset.get_DS(stage='train', labelled='yes')\n",
    "set_1 = Dataset.get_DS(stage='train', labelled='no')\n",
    "set_2 = Dataset.get_DS(stage='test', labelled='no')\n",
    "set_1.append(set_2.data)\n",
    "set_1.append(train_set.data)\n",
    "emb_set = set_1\n",
    "print(emb_set.size, train_set.size, validation_set.size, test_set.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Load Complete\n"
     ]
    }
   ],
   "source": [
    "#emb_set.process_for_embedding()\n",
    "#sentences = emb_set.get_sentences()\n",
    "#fn.write_sentences(sentences, 'final_meta/sentences')\n",
    "sentences = fn.load_sentences('final_meta/sentences')\n",
    "\n",
    "#model = Word2Vec(sentences, min_count=1, size=100)\n",
    "#model.save('final_meta/W2V')\n",
    "model = Word2Vec.load('final_meta/W2V')\n",
    "\n",
    "vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer and Index Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indices Load Complete\n",
      "Embedding Layer Load Complete\n"
     ]
    }
   ],
   "source": [
    "#word_indices, emb_layer = fn.get_index_and_emb_layer(model)\n",
    "#fn.write_word_indices(word_indices, 's2s/word_indices')\n",
    "#fn.write_emb_layer(emb_layer, 's2s/emb_layer')\n",
    "\n",
    "word_indices = fn.load_word_indices('s2s/word_indices')\n",
    "emb_layer = fn.load_emb_layer('s2s/emb_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELS2S Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.process_for_els2s_testing()\n",
    "validation_set.process_for_els2s_testing()\n",
    "test_set.process_for_els2s_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insulin', 'nph']\n",
      "2\n",
      "['inr', 'of', '<num>.<num>', ',', 'last', 'target', '<num>.<num>', ',', 'then', 'received', '<num>', 'mg', 'in', 'evening', 'x', '<num>', 'additionally', ',', 'percocet', '<num>-<num>', 'tablets', 'po', 'q', '<num>', 'prn', ',', 'colace', '<num>', 'mg', 'po', 'bid', ',', 'insulin', 'nph', '<num>', 'units', 'subcu', 'bid', ',', 'sliding', 'scale', 'insulin', 'subcu', 'q', '<num>', ',', 'isordil', '<num>', 'mg', 'tid', ',', 'zestril', '<num>', 'mg', 'q', 'd', ',', 'lopressor', '<num>', 'mg', 'bid', ',', 'axid', '<num>', 'mg', 'po', 'bid', 'she', 'will', 'follow', 'with', 'dr', 'noah', 'in']\n",
      "74\n",
      "[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [2], [2], [4], [4], [4], [4], [0], [0], [1], [2], [2], [3], [4], [4], [4], [0], [1], [2], [2], [3], [4], [0], [1], [1], [2], [2], [3], [4], [0], [2], [2], [1], [3], [4], [4], [0], [1], [2], [2], [4], [0], [1], [2], [2], [4], [4], [0], [1], [2], [2], [4], [0], [1], [2], [2], [3], [4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "94\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "entry = 1\n",
    "\n",
    "print(test_set.data[0].inp_toks[entry])\n",
    "print(len(test_set.data[0].inp_toks[entry]))\n",
    "print(test_set.data[0].inp_words[entry])\n",
    "print(len(test_set.data[0].inp_words[entry]))\n",
    "print(test_set.data[0].inp_labels[entry])\n",
    "print(len(test_set.data[0].inp_labels[entry]))\n",
    "print(test_set.data[0].out_labels[entry])\n",
    "print(len(test_set.data[0].out_labels[entry]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inr of <num>.<num> , last target <num>.<num> , then received <num> mg in evening x <num> additionally , percocet <num>-<num> tablets po q <num> prn , colace <num> mg po bid , insulin nph <num> units subcu bid , sliding scale insulin subcu q <num> , isordil <num> mg tid , zestril <num> mg q d , lopressor <num> mg bid , axid <num> mg po bid she will follow with dr noah in\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(test_set.data[0].inp_words[entry]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 94\n"
     ]
    }
   ],
   "source": [
    "max_tok, max_inp = 0, 0\n",
    "for setin in [train_set, validation_set, test_set]:\n",
    "    for case in setin.data:\n",
    "        for inp_words in case.inp_words:\n",
    "            max_inp = max(max_inp, len(inp_words))\n",
    "        for inp_tok in case.inp_toks:\n",
    "            max_tok = max(max_tok, len(inp_tok))\n",
    "print(max_tok, max_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {}\n",
    "sets['train'] = train_set.get_els2s_sets(word_indices, max_tok, max_inp)\n",
    "sets['validation'] = validation_set.get_els2s_sets(word_indices, max_tok, max_inp)\n",
    "sets['test'] = test_set.get_els2s_sets(word_indices, max_tok, max_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26054, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[8424, 34144, 24390, 15338, 5944, 24390, 39594, 48602, 35529, 12093, 27193, 24390, 5702, 5944, 4324, 4560, 34798, 35529, 29647, 20302, 12646, 24390, 24379, 39594, 11567, 19877, 9609, 11228, 26054, 5702, 16969, 1848, 20418, 41595, 4560, 42508, 5702, 16969, 14636, 19084, 26517, 39692, 673, 12646, 10311, 15131, 35388, 15448, 47267, 45997, 48602, 19089, 4560, 2106, 35626, 42958, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "94\n",
      "[[6], [6], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [2], [2], [3], [4], [4], [0], [1], [2], [2], [3], [4], [0], [0], [1], [0], [0], [5], [5], [5], [5], [5], [5], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "94\n",
      "56\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "entry = 19\n",
    "\n",
    "print(sets['test'][0][19])\n",
    "print(sets['test'][1][19])\n",
    "print(len(sets['test'][1][19]))\n",
    "print(sets['test'][2][19])\n",
    "print(len(sets['test'][2][19]))\n",
    "print(sets['test'][3][19])\n",
    "print(sets['test'][4][19])\n",
    "print(sets['test'][5][19])\n",
    "print(len(sets['test'][5][19]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 1, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "sk.metrics.f1_score(sets['test'][4][0], pred, labels=[1, 2, 3, 4, 5, 6], average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "\n",
    "from Iterator import ELS2SIterator\n",
    "\n",
    "\n",
    "class ELS2S_Model:\n",
    "\n",
    "    def __init__(self, \n",
    "                 decay=0,\n",
    "                 batch=50,\n",
    "                 enc_vocab_size=100, \n",
    "                 dec_vocab_size=9, \n",
    "                 enc_emb_size=100, \n",
    "                 dec_emb_size=100, \n",
    "                 state_size=100, \n",
    "                 dropout=1.0,\n",
    "                 learn_rate=0.001,\n",
    "                 max_gradient_norm=5,\n",
    "                 enc_emb_layer=False):\n",
    "        self.decay = decay\n",
    "        self.batch = batch\n",
    "        self.enc_vocab_size = enc_vocab_size\n",
    "        self.dec_vocab_size = dec_vocab_size\n",
    "        self.enc_emb_size = enc_emb_size\n",
    "        self.dec_emb_size = dec_emb_size\n",
    "        self.state_size = state_size\n",
    "        self.dropout = dropout\n",
    "        self.learn_rate = learn_rate\n",
    "        self.max_gradient_norm=max_gradient_norm\n",
    "        self.enc_emb_layer = enc_emb_layer\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "\n",
    "    def reset_graph(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.reset_graph()\n",
    "\n",
    "        # Placeholders\n",
    "        targs = tf.placeholder(dtype=tf.int32, shape=[self.batch, 14])  # [batch_size, num_steps]\n",
    "        toks = tf.placeholder(dtype=tf.int32, shape=[self.batch, 94])\n",
    "        labs = tf.placeholder(dtype=tf.float32, shape=[self.batch, 94, 1])\n",
    "        inp_seqlen = tf.placeholder(tf.int32, shape=[self.batch])\n",
    "        y = tf.placeholder(dtype=tf.int32, shape=[self.batch, 94])\n",
    "        #target_weights = tf.placeholder(tf.float32, shape=[self.batch, 94])\n",
    "        #keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Embeddings\n",
    "        if type(self.enc_emb_layer) != bool:\n",
    "            enc_embeddings = tf.get_variable('enc_embedding_matrix', [self.enc_vocab_size, self.enc_emb_size], dtype=tf.float32)\n",
    "        else:\n",
    "            enc_embeddings = tf.get_variable('enc_embedding_matrix', initializer=tf.constant(self.enc_emb_layer))\n",
    "        taremb = tf.nn.embedding_lookup(enc_embeddings, targs)\n",
    "        tarbow = tf.reduce_sum(taremb, 1)\n",
    "        tarbowlab = tf.concat([tarbow, tf.constant(1, dtype=tf.float32, shape=[self.batch, 1])], axis=-1)\n",
    "        \n",
    "        tokind = tf.nn.embedding_lookup(enc_embeddings, toks)\n",
    "        rnn_inputs = tf.concat([tokind, labs], axis=-1)\n",
    "        \n",
    "        # Bidirectional\n",
    "        forward_cell = tf.nn.rnn_cell.GRUCell(self.state_size)\n",
    "        #fw_zero_state = forward_cell.zero_state(batch_size=self.batch, dtype=tf.float32)\n",
    "        #fw_init_state = fw_zero_state.clone(cell_state=tarbowlab)\n",
    "        \n",
    "        backward_cell = tf.nn.rnn_cell.GRUCell(self.state_size)\n",
    "        #bw_zero_state = backward_cell.zero_state(batch_size=self.batch, dtype=tf.float32)\n",
    "        #bw_init_state = bw_zero_state.clone(cell_state=tarbowlab)\n",
    "\n",
    "        bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_cell,\n",
    "                                                                    cell_bw=backward_cell, \n",
    "                                                                    inputs=rnn_inputs,\n",
    "                                                                    sequence_length=inp_seqlen,\n",
    "                                                                    initial_state_fw=tarbowlab,\n",
    "                                                                    initial_state_bw=tarbowlab,\n",
    "                                                                    dtype=tf.float32)\n",
    "        \n",
    "        # Conbining the output hidden states of ceells\n",
    "        rnn_outputs = tf.concat(bi_outputs, -1)\n",
    "        \n",
    "        with tf.variable_scope('softmax'):\n",
    "            W = tf.Variable(tf.truncated_normal(shape=[2*self.state_size, 7], stddev=0.05))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[7]))\n",
    "        new_rnn_shape = tf.reshape(rnn_outputs, shape=[-1, 2*self.state_size])\n",
    "        logits = tf.matmul(new_rnn_shape, W) + b\n",
    "        output = tf.reshape(logits, [self.batch, 94, 7])  \n",
    "        \n",
    "        preds = tf.nn.softmax(output)       \n",
    "        prediction = tf.argmax(preds, 2)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "        train_step = tf.train.AdamOptimizer(self.learn_rate).minimize(loss)\n",
    "\n",
    "        self.graph = {'targs': targs,\n",
    "                      'toks': toks,\n",
    "                      'labs': labs,\n",
    "                      'inp_seqlen': inp_seqlen,\n",
    "                      'y': y,\n",
    "                      'prediction': prediction,\n",
    "                      'loss': loss,\n",
    "                      'ts': train_step\n",
    "                      }\n",
    "\n",
    "    def train(self, sets, epochs=10, report_percentage=1, show_progress=False, show_plot=False):\n",
    "        # Start a tf session and run the optimisation algorithm\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        trainer = ELS2SIterator(*(sets['train'][:-1]))\n",
    "\n",
    "        #train_truth = sets['train'][4]\n",
    "        #validation_truth = sets['validation'][4]\n",
    "        #test_truth = sets['validation'][4]\n",
    "\n",
    "        #train_feed = {self.graph['enc_x']: sets['train'][0],\n",
    "                      #self.graph['enc_labs']: sets['train'][1],\n",
    "                      #self.graph['enc_seqlen']: sets['train'][2],\n",
    "                      #self.graph['dec_x']: sets['train'][3],\n",
    "                      #self.graph['dec_seqlen']: sets['train'][4],\n",
    "                      #self.graph['y']: sets['train'][5],\n",
    "                      #self.graph['target_weights']: sets['train'][6],\n",
    "                      #self.graph['keep_prob']: 1.0}\n",
    "        #validation_feed = {self.graph['enc_x']: sets['validation'][0],\n",
    "                           #self.graph['enc_labs']: sets['validation'][1],\n",
    "                           #self.graph['enc_seqlen']: sets['validation'][2],\n",
    "                           #self.graph['dec_x']: sets['validation'][3],\n",
    "                           #self.graph['dec_seqlen']: sets['validation'][4],\n",
    "                           #self.graph['y']: sets['validation'][5],\n",
    "                           #self.graph['target_weights']: sets['validation'][6],\n",
    "                           #self.graph['keep_prob']: 1.0}\n",
    "        #test_feed = {self.graph['enc_x']: sets['test'][0],\n",
    "                     #self.graph['enc_labs']: sets['test'][1],\n",
    "                     #self.graph['enc_seqlen']: sets['test'][2],\n",
    "                     #self.graph['dec_x']: sets['test'][3],\n",
    "                     #self.graph['dec_seqlen']: sets['test'][4],\n",
    "                     #self.graph['y']: sets['test'][5],\n",
    "                     #self.graph['target_weights']: sets['test'][6],\n",
    "                     #self.graph['keep_prob']: 1.0}\n",
    "\n",
    "        #train_f1_score = []\n",
    "        #validation_f1_score = []\n",
    "\n",
    "        mark = (epochs * (len(sets['train'][0]) // self.batch) * report_percentage) // 100\n",
    "        check_point = []\n",
    "        N = 0\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        while trainer.epochs < epochs:\n",
    "            trtar, trtok, trlab, trlen, trout = trainer.next_batch(self.batch)\n",
    "            feed = {self.graph['targs']: trtar,\n",
    "                    self.graph['toks']: trtok,\n",
    "                    self.graph['labs']: trlab, \n",
    "                    self.graph['inp_seqlen']: trlen,\n",
    "                    self.graph['y']: trout}\n",
    "            if N % mark == 0:\n",
    "                #prediction = self.sess.run(self.graph['prediction'], feed_dict=train_feed)\n",
    "                #pred_cut = [pred[:end] for (pred, end) in zip(prediction, sets['train'][3])]\n",
    "                #f1_sum = 0\n",
    "                #for i in range(len(pred_cut)):\n",
    "                #    f1_sum += sk.metrics.f1_score(train_truth[i], pred_cut[i], labels=[1, 2, 3, 4, 5, 6], average='micro')       \n",
    "                #train_f1_score.append(f1_sum / len(pred_cut))\n",
    "                #prediction = self.sess.run(self.graph['prediction'], feed_dict=validation_feed)\n",
    "                #pred_cut = [pred[:end] for (pred, end) in zip(prediction, sets['validation'][3])]\n",
    "                #f1_sum = 0\n",
    "                #for i in range(len(pred_cut)):\n",
    "                #    f1_sum += sk.metrics.f1_score(validation_truth[i], pred_cut[i], labels=[1, 2, 3, 4, 5, 6], average='micro')       \n",
    "                #validation_f1_score.append(f1_sum / len(pred_cut))\n",
    "                #check_point.append(N)\n",
    "                loss = self.sess.run(self.graph['loss'], feed_dict=feed)\n",
    "                print('Epoch: {}, Learn Rate: {:.7f}, Perplexity: {:.2f}'.format(trainer.epochs, self.learn_rate, loss))\n",
    "                if show_progress: print(\"Progress: %d%%\" % (N * report_percentage // mark), end=\"\\r\")\n",
    "            self.sess.run(self.graph['ts'], feed_dict=feed)\n",
    "            #self.learn_rate = self.learn_rate * 1/(1 + self.decay * trainer.epochs)\n",
    "            N += 1\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "        #test_prediction = self.sess.run(self.graph['prediction'], feed_dict=test_feed)\n",
    "        #pred_cut = [pred[:end] for (pred, end) in zip(prediction, sets['test'][3])]\n",
    "        #f1_sum = 0\n",
    "        #for i in range(len(pred_cut)):\n",
    "        #    f1_sum += sk.metrics.f1_score(test_truth[i], pred_cut[i], labels=[1, 2, 3, 4, 5, 6], average='micro')       \n",
    "        #test_f1_score = f1_sum / len(pred_cut)\n",
    "        \n",
    "        #if show_progress:\n",
    "        #    print('FInal Values: Tr-F1: {:.4f}, Val-F1: {:.4f}'.format(train_f1_score[-1], validation_f1_score[-1]))\n",
    "        #    print(\"Test F1-Score: {:.4f}\\n\".format(test_f1_score))\n",
    "        \n",
    "        #if show_plot:\n",
    "        #    np_check_point = np.array(check_point)\n",
    "        #    np_train_f1 = np.array(train_f1_score)\n",
    "        #    np_val_f1 = np.array(validation_f1_score)\n",
    "\n",
    "        #    plt.plot(np_check_point, np_train_f1, label=\"Train\")\n",
    "        #    plt.plot(np_check_point, np_val_f1, label=\"Validation\")\n",
    "        #    plt.plot(np_check_point, np.ones(len(np_check_point))*0.35, label=\"Baseline\")\n",
    "        #    plt.xlabel(\"Batches\")\n",
    "        #    plt.ylabel(\"F1-Score\")\n",
    "        #    plt.legend()\n",
    "        #    plt.show()\n",
    "        \n",
    "        #return train_f1_score, validation_f1_score, test_f1_score\n",
    "\n",
    "    def predict(self, data):\n",
    "        feed = {self.graph['targs']: data[0],\n",
    "                self.graph['toks']: data[1],\n",
    "                self.graph['labs']: data[2], \n",
    "                self.graph['y']: data[4], \n",
    "                self.graph['inp_seqlen']: data[3]}\n",
    "        return self.sess.run(self.graph['prediction'], feed_dict=feed)\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Learn Rate: 0.0010000, Perplexity: 1.94\n",
      "Epoch: 1, Learn Rate: 0.0010000, Perplexity: 0.93\n",
      "Epoch: 2, Learn Rate: 0.0010000, Perplexity: 0.81\n",
      "Epoch: 3, Learn Rate: 0.0010000, Perplexity: 0.67\n",
      "Epoch: 4, Learn Rate: 0.0010000, Perplexity: 0.57\n",
      "Epoch: 5, Learn Rate: 0.0010000, Perplexity: 0.54\n",
      "Epoch: 6, Learn Rate: 0.0010000, Perplexity: 0.43\n",
      "Epoch: 7, Learn Rate: 0.0010000, Perplexity: 0.36\n",
      "Epoch: 8, Learn Rate: 0.0010000, Perplexity: 0.29\n",
      "Epoch: 9, Learn Rate: 0.0010000, Perplexity: 0.27\n",
      "Epoch: 10, Learn Rate: 0.0010000, Perplexity: 0.22\n",
      "Epoch: 11, Learn Rate: 0.0010000, Perplexity: 0.18\n",
      "Epoch: 12, Learn Rate: 0.0010000, Perplexity: 0.17\n",
      "Epoch: 13, Learn Rate: 0.0010000, Perplexity: 0.15\n",
      "Epoch: 14, Learn Rate: 0.0010000, Perplexity: 0.12\n",
      "Epoch: 15, Learn Rate: 0.0010000, Perplexity: 0.12\n",
      "Epoch: 16, Learn Rate: 0.0010000, Perplexity: 0.10\n",
      "Epoch: 17, Learn Rate: 0.0010000, Perplexity: 0.08\n",
      "Epoch: 18, Learn Rate: 0.0010000, Perplexity: 0.07\n",
      "Epoch: 19, Learn Rate: 0.0010000, Perplexity: 0.07\n",
      "Epoch: 20, Learn Rate: 0.0010000, Perplexity: 0.06\n",
      "Epoch: 21, Learn Rate: 0.0010000, Perplexity: 0.06\n",
      "Epoch: 22, Learn Rate: 0.0010000, Perplexity: 0.05\n",
      "Epoch: 23, Learn Rate: 0.0010000, Perplexity: 0.04\n",
      "Epoch: 24, Learn Rate: 0.0010000, Perplexity: 0.04\n",
      "Epoch: 25, Learn Rate: 0.0010000, Perplexity: 0.03\n",
      "Epoch: 26, Learn Rate: 0.0010000, Perplexity: 0.03\n",
      "Epoch: 27, Learn Rate: 0.0010000, Perplexity: 0.03\n",
      "Epoch: 28, Learn Rate: 0.0010000, Perplexity: 0.02\n",
      "Epoch: 29, Learn Rate: 0.0010000, Perplexity: 0.04\n",
      "Epoch: 30, Learn Rate: 0.0010000, Perplexity: 0.03\n",
      "Epoch: 31, Learn Rate: 0.0010000, Perplexity: 0.02\n",
      "Epoch: 32, Learn Rate: 0.0010000, Perplexity: 0.02\n",
      "Epoch: 33, Learn Rate: 0.0010000, Perplexity: 0.02\n",
      "Epoch: 34, Learn Rate: 0.0010000, Perplexity: 0.02\n",
      "Epoch: 35, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 36, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 37, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 38, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 39, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 40, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 41, Learn Rate: 0.0010000, Perplexity: 0.02\n",
      "Epoch: 42, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 43, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 44, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 45, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 46, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 47, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 48, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 49, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 50, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 51, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 52, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 53, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 54, Learn Rate: 0.0010000, Perplexity: 0.01\n",
      "Epoch: 55, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 56, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 57, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 58, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 59, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 60, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 61, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 62, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 63, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 64, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 65, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 66, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 67, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 68, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 69, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 70, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 71, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 72, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 73, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 74, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 75, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 76, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 77, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 78, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 79, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 80, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 81, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 82, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 83, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 84, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 85, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 86, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 87, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 88, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 89, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 90, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 91, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 92, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 93, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 94, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 95, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 96, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 97, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 98, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 99, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Epoch: 100, Learn Rate: 0.0010000, Perplexity: 0.00\n",
      "Progress: 100%\r"
     ]
    }
   ],
   "source": [
    "ELS2S = ELS2S_Model(decay = 0.000025,\n",
    "                batch=50,\n",
    "                enc_vocab_size=len(word_indices), \n",
    "                dec_vocab_size=len(word_indices), \n",
    "                enc_emb_size=100, \n",
    "                dec_emb_size=100, \n",
    "                state_size=101, \n",
    "                dropout=1.0,\n",
    "                learn_rate=0.001,\n",
    "                max_gradient_norm=5,\n",
    "                enc_emb_layer=emb_layer)\n",
    "ELS2S.build_graph()\n",
    "ELS2S.train(sets=sets, epochs=100, report_percentage=1, show_progress=True, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tru_seqs = [label for entry in sets['test'][4] for label in entry]\n",
    "#for i in range(len(sets['test'][5])):\n",
    "#    tru_vocab.append(sets['test'][5][i][:sets['test'][4][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(sets['test'][0])):\n",
    "    dummy = [[], [], [], [], [], []]\n",
    "    for j in range(50):\n",
    "        for k in range(6):\n",
    "            dummy[k].append(sets['test'][k][i])\n",
    "    temp = ELS2S.predict(dummy)\n",
    "    res.append(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_seqs = [label for entry in res for label in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82042494859492809"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk.metrics.f1_score(tru_seqs, res_seqs, labels=[1, 2, 3, 4, 5, 6], average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_seq_m = [1 if label==1 else 0 for label in tru_seqs]\n",
    "tru_seq_do = [1 if label==2 else 0 for label in tru_seqs]\n",
    "tru_seq_mo = [1 if label==3 else 0 for label in tru_seqs]\n",
    "tru_seq_f = [1 if label==4 else 0 for label in tru_seqs]\n",
    "tru_seq_du = [1 if label==5 else 0 for label in tru_seqs]\n",
    "tru_seq_r = [1 if label==6 else 0 for label in tru_seqs]\n",
    "\n",
    "res_seq_m = [1 if label==1 else 0 for label in res_seqs]\n",
    "res_seq_do = [1 if label==2 else 0 for label in res_seqs]\n",
    "res_seq_mo = [1 if label==3 else 0 for label in res_seqs]\n",
    "res_seq_f = [1 if label==4 else 0 for label in res_seqs]\n",
    "res_seq_du = [1 if label==5 else 0 for label in res_seqs]\n",
    "res_seq_r = [1 if label==6 else 0 for label in res_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med F1-Score: 0.892\n",
      "Dos F1-Score: 0.783\n",
      "Mod F1-Score: 0.840\n",
      "Fre F1-Score: 0.789\n",
      "Dur F1-Score: 0.764\n",
      "Rea F1-Score: 0.720\n"
     ]
    }
   ],
   "source": [
    "print('Med F1-Score: {:.3f}'.format(sk.metrics.f1_score(tru_seq_m, res_seq_m, average='binary')))\n",
    "print('Dos F1-Score: {:.3f}'.format(sk.metrics.f1_score(tru_seq_do, res_seq_do, average='binary')))\n",
    "print('Mod F1-Score: {:.3f}'.format(sk.metrics.f1_score(tru_seq_mo, res_seq_mo, average='binary')))\n",
    "print('Fre F1-Score: {:.3f}'.format(sk.metrics.f1_score(tru_seq_f, res_seq_f, average='binary')))\n",
    "print('Dur F1-Score: {:.3f}'.format(sk.metrics.f1_score(tru_seq_du, res_seq_du, average='binary')))\n",
    "print('Rea F1-Score: {:.3f}'.format(sk.metrics.f1_score(tru_seq_r, res_seq_r, average='binary')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units\t0\t0\n",
      "subcu\t0\t0\n",
      "q\t0\t0\n",
      "pm\t0\t0\n",
      "supplemented\t0\t0\n",
      "by\t0\t0\n",
      "sliding\t0\t0\n",
      "scale\t0\t0\n",
      "regular\t0\t0\n",
      "insulin\t0\t0\n",
      "scale\t0\t0\n",
      ",\t0\t0\n",
      "isordil\t0\t0\n",
      "<num>\t0\t0\n",
      "mg\t0\t0\n",
      "tid\t0\t0\n",
      ",\t0\t0\n",
      "zestril\t0\t0\n",
      "<num>\t0\t0\n",
      "mg\t0\t0\n",
      "q\t0\t0\n",
      "d\t0\t0\n",
      ",\t0\t0\n",
      "lopressor\t0\t0\n",
      "<num>\t0\t0\n",
      "mg\t0\t0\n",
      "bid\t0\t0\n",
      ",\t0\t0\n",
      "axid\t0\t0\n",
      "<num>\t0\t0\n",
      "mg\t0\t0\n",
      "bid\t0\t0\n",
      ",\t0\t0\n",
      "ofloxacin\t1\t1\n",
      "<num>\t2\t2\n",
      "mg\t2\t2\n",
      "po\t3\t3\n",
      "q\t4\t4\n",
      "<num>\t4\t4\n",
      ",\t0\t0\n",
      "ecotrin\t0\t0\n",
      "<num>\t0\t0\n",
      "mg\t0\t0\n",
      "q\t0\t0\n",
      "d\t0\t0\n",
      ",\t0\t0\n",
      "vancomycin\t0\t0\n",
      "<num>\t0\t0\n",
      "mg\t0\t0\n",
      "q\t0\t0\n",
      "<num>.\t0\t0\n",
      "she\t0\t0\n",
      "reports\t0\t0\n",
      "an\t0\t0\n",
      "allergy\t0\t0\n",
      "to\t0\t0\n",
      "codeine\t0\t0\n",
      "and\t0\t0\n",
      "iodine\t0\t0\n"
     ]
    }
   ],
   "source": [
    "bla = 0\n",
    "\n",
    "for i in range(len(test_set.data[0].inp_words[bla])):\n",
    "    print('{}\\t{}\\t{}'.format(test_set.data[0].inp_words[bla][i], sets['test'][4][bla][i], res[bla][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_vocab = res\n",
    "res_words = []\n",
    "for case in res_vocab:\n",
    "    temp = []\n",
    "    for index in case:\n",
    "        for word in word_indices:\n",
    "            if word_indices[word] == index:\n",
    "                temp.append(word)\n",
    "    res_words.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(360):\n",
    "    if len(res_words[i]) != len(res_vocab[i]):\n",
    "        print('nooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ofloxacin', '<num>', 'mg', 'po', 'q', '<num>', '<eos>']\n",
      "['ofloxacin', '<num>', 'mg', 'po', 'q', '<num>', '<eos>']\n",
      "['insulin', 'nph', '<num>', 'units', 'subcu', 'bid', '<eos>']\n",
      "['insulin', 'nph', '<num>', 'units', 'subcu', 'bid', '<eos>']\n",
      "['colace', '<num>', 'mg', 'po', 'bid', '<eos>']\n",
      "['colace', '<num>', 'mg', 'po', 'bid', '<eos>']\n",
      "['percocet', '<num>-<num>', 'tablets', 'po', 'q', '<num>', 'prn', '<eos>']\n",
      "['percocet', '<num>-<num>', 'tablets', 'po', 'q', '<num>', 'prn', '<eos>']\n",
      "['coumadin', '<eos>']\n",
      "['coumadin', '<num>', 'mg', 'in', 'evening', 'x', '<num>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(res_words[i])\n",
    "    print(tru_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s2s/model/model.ckpt'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(S2S.sess, \"s2s/model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from s2s/model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "loaded = S2S_Model(decay = 0.00001,\n",
    "                batch=50,\n",
    "                enc_vocab_size=len(word_indices), \n",
    "                dec_vocab_size=len(word_indices), \n",
    "                enc_emb_size=100, \n",
    "                dec_emb_size=100, \n",
    "                state_size=128, \n",
    "                dropout=1.0,\n",
    "                learn_rate=0.001,\n",
    "                max_gradient_norm=5,\n",
    "                enc_emb_layer=emb_layer)\n",
    "loaded.build_graph()\n",
    "loaded.sess = tf.Session()\n",
    "loader = tf.train.Saver()\n",
    "loader.restore(loaded.sess, \"s2s/model/model.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
