{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import Functions as fn\n",
    "import Iterator as it\n",
    "from DS import DS\n",
    "from Set import pool\n",
    "from FFModel import FF_Model\n",
    "from RNNModel import RNN_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Load Complete\n",
      "Raw Labels Load Complete\n"
     ]
    }
   ],
   "source": [
    "Dataset = pool()\n",
    "Dataset.load_texts('raw_texts')\n",
    "Dataset.load_labels('raw_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Load Complete\n"
     ]
    }
   ],
   "source": [
    "target_dict = fn.load_labels('final_meta/labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585 238 10 10\n"
     ]
    }
   ],
   "source": [
    "train_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[:-10])\n",
    "validation_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[-10:])\n",
    "test_set = Dataset.get_DS(stage='train', labelled='yes')\n",
    "set_1 = Dataset.get_DS(stage='train', labelled='no')\n",
    "set_2 = Dataset.get_DS(stage='test', labelled='no')\n",
    "set_1.append(set_2.data)\n",
    "set_1.append(train_set.data)\n",
    "emb_set = set_1\n",
    "print(emb_set.size, train_set.size, validation_set.size, test_set.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Load Complete\n"
     ]
    }
   ],
   "source": [
    "#emb_set.process_for_embedding()\n",
    "#sentences = emb_set.get_sentences()\n",
    "#fn.write_sentences(sentences, 'final_meta/sentences')\n",
    "sentences = fn.load_sentences('final_meta/sentences')\n",
    "\n",
    "#model = Word2Vec(sentences, min_count=1, size=100)\n",
    "#model.save('final_meta/W2V')\n",
    "model = Word2Vec.load('final_meta/W2V')\n",
    "\n",
    "vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer and Index Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indices Load Complete\n",
      "Embedding Layer Load Complete\n"
     ]
    }
   ],
   "source": [
    "#word_indices, emb_layer = fn.get_index_and_emb_layer(model)\n",
    "#fn.write_word_indices(word_indices, 'final_meta/word_indices')\n",
    "#fn.write_emb_layer(emb_layer, 'final_meta/emb_layer')\n",
    "\n",
    "word_indices = fn.load_word_indices('final_meta/word_indices')\n",
    "emb_layer = fn.load_emb_layer('final_meta/emb_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'r'\n",
    "\n",
    "train_set.process_for_testing(target)\n",
    "validation_set.process_for_testing(target)\n",
    "test_set.process_for_testing(target)\n",
    "\n",
    "lw, rw = 5, 5\n",
    "sets = {}\n",
    "sets['train_set'], sets['train_labels'], _, sets['train_lengths'] = train_set.get_rnn_sets(word_indices, lw, rw)\n",
    "sets['validation_set'], sets['validation_labels'], _, sets['validation_lengths']= validation_set.get_rnn_sets(word_indices, lw, rw)\n",
    "sets['test_set'], sets['test_labels'], sets['test_words'], sets['test_lengths'] = test_set.get_rnn_sets(word_indices, lw, rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: med: 0.01% non-med: 0.99%\n",
      "Ratio: med: 0.10% non-med: 0.90%\n"
     ]
    }
   ],
   "source": [
    "print('Ratio: med: {:.2f}% non-med: {:.2f}%'.format(*(np.array(sets['train_labels']).sum(0)/ len(sets['train_labels']))))\n",
    "fn.saturate_training_set(sets, 0.1, seqlen=True)\n",
    "print('Ratio: med: {:.2f}% non-med: {:.2f}%'.format(*(np.array(sets['train_labels']).sum(0)/ len(sets['train_labels']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "c:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "RNN = RNN_Model(vocab_size=len(word_indices), state_size=128, num_classes=2, dropout=0.5, learn_rate=0.001, emb_layer=emb_layer)\n",
    "RNN.build_graph()\n",
    "results_list = RNN.train(sets=sets, epochs=3, batch=50, report_percentage=1, show_progress=True, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = RNN.predict(sets['test_set'], sets['test_lengths'])\n",
    "tru = np.argmax(sets['test_labels'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_perf = fn.token_perf(res, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [0 if word in target_dict['reasons'] else 1 for word in sets['test_words']]\n",
    "sk.metrics.f1_score(tru, baseline, pos_label=0, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.category_words(sets['test_words'], res, tru, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn.colour_text(sets['test_words'], res, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testers = Dataset.get_DS(stage='train', labelled='yes')\n",
    "phrase_perf = fn.phrase_perf(target, RNN, testers, word_indices, side_words=[lw, rw], tfpn=True, show_phrases=True, case_info=True, rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_sizes = [100]\n",
    "emb_models = 1\n",
    "target_saturations = [0.05, 0.1, 0.2, 0.5, 0.7]\n",
    "layer_sizes = [50]\n",
    "dropouts = [1.0]\n",
    "learn_rates = [0.01]\n",
    "epochs = [100]\n",
    "NN_num = 5\n",
    "\n",
    "case_num = len(emb_sizes)*emb_models*len(layer_sizes)*len(target_saturations)*len(epochs)*len(dropouts)*len(learn_rates)*NN_num\n",
    "print(case_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_performance = 0\n",
    "n = 1\n",
    "\n",
    "for emb_size in emb_sizes:\n",
    "    print('Model Number: %d/%d' %(n, case_num))\n",
    "    for i in range(emb_models):\n",
    "        model = Word2Vec(sentences, min_count=1, size=emb_size)\n",
    "        for saturation in target_saturations:\n",
    "            sets = fn.get_traintest2 (labelled_cases, model)\n",
    "            fn.saturate_training_set(sets, model, target_dict['medications'], saturation)\n",
    "            for layer_size in layer_sizes:\n",
    "                for drop in dropouts:\n",
    "                    for rate in learn_rates:\n",
    "                        for epoch in epochs:\n",
    "                            for j in range(NN_num):\n",
    "                                print('Model Number: %d/%d' %(n, case_num))\n",
    "                                print('ES: %d EM: %d sat: %f, LS: %d, drop: %f, LR: %f, epochs: %d, NN: %d' \\\n",
    "                                       % (emb_size, i, saturation, layer_size, drop, rate, epoch, j))\n",
    "                                NN = FF_Model(input_size=emb_size, layers=[layer_size], dropout=drop, learn_rate=rate)\n",
    "                                NN.build_graph()\n",
    "                                NN.train(sets, epochs=epoch)\n",
    "                                res = NN.predict(sets['test_set'])\n",
    "                                tru = np.argmax(sets['test_labels'], 1)\n",
    "                                perf = sk.metrics.f1_score(tru, res, pos_label=0)\n",
    "                                if perf > max_performance:\n",
    "                                    max_performance = perf\n",
    "                                    NN.save_model('gold')\n",
    "                                    model.save('gold/GOLDEMB')\n",
    "                                NN.close()\n",
    "                                n += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
