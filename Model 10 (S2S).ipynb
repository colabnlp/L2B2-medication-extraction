{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import Functions as fn\n",
    "import Iterator as it\n",
    "from DS import DS\n",
    "from Set import pool\n",
    "from FFModel import FF_Model\n",
    "from RNNModel import RNN_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Load Complete\n",
      "Raw Labels Load Complete\n"
     ]
    }
   ],
   "source": [
    "Dataset = pool()\n",
    "Dataset.load_texts('raw_texts')\n",
    "Dataset.load_labels('raw_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Load Complete\n"
     ]
    }
   ],
   "source": [
    "target_dict = fn.load_labels('labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Load Complete\n"
     ]
    }
   ],
   "source": [
    "#Dataset.process_for_embedding()\n",
    "#sentences = Dataset.get_sentences()\n",
    "#fn.write_sentences('sentences')\n",
    "sentences = fn.load_sentences('sentences')\n",
    "\n",
    "#model = Word2Vec(sentences, min_count=1, size=100)\n",
    "#model.save('W2V')\n",
    "model = Word2Vec.load('W2V')\n",
    "\n",
    "vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer and Index Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indices Load Complete\n",
      "Embedding Layer Load Complete\n"
     ]
    }
   ],
   "source": [
    "#word_indices, emb_layer = fn.get_index_and_emb_layer(model)\n",
    "#fn.write_word_indices(word_indices, 'word_indices')\n",
    "#fn.write_emb_layer(emb_layer, 'emb_layer')\n",
    "\n",
    "word_indices = fn.load_word_indices('word_indices')\n",
    "emb_layer = fn.load_emb_layer('emb_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [ -3.28335352e-02   8.42782389e-03   3.92741375e-02  -1.82002522e-02\n",
      "    2.07882039e-02  -9.61498916e-03   1.28974142e-02  -8.05556832e-04\n",
      "    6.30482053e-03   7.94884656e-03   5.00391871e-02  -7.92084336e-02\n",
      "    1.74176935e-02   4.36509140e-02  -5.06037381e-03  -8.04699305e-03\n",
      "    1.23671414e-02  -2.82115303e-03  -1.99902821e-02   1.65354107e-02\n",
      "    7.17794523e-02  -1.93883311e-02   1.06656607e-02   3.10288686e-02\n",
      "    1.37773687e-02  -2.24238820e-02  -8.62452295e-03  -3.20653580e-02\n",
      "   -2.26410348e-02   1.89997889e-02   3.81298214e-02  -1.57296620e-02\n",
      "   -8.18264484e-03  -6.45961380e-04   1.64815430e-02   1.89289427e-03\n",
      "   -3.98474596e-02   3.22238146e-03  -3.13322234e-05   2.28157081e-02\n",
      "   -3.23769301e-02   3.41075361e-02  -5.19410111e-02   4.66750599e-02\n",
      "    5.59318066e-02  -1.14701036e-02   1.86952129e-02   2.74178982e-02\n",
      "   -4.36648633e-03  -4.12623361e-02   3.64324264e-02   7.25180469e-03\n",
      "   -3.19332108e-02  -4.53153849e-02  -3.22421640e-02   6.57228008e-02\n",
      "    1.20110651e-02   1.76546481e-02  -3.41002084e-02   9.47675109e-03\n",
      "    2.46836916e-02  -1.01987617e-02  -9.73427761e-03  -1.16602071e-02\n",
      "   -5.29632233e-02   2.67814323e-02   1.77522190e-02   6.56463497e-04\n",
      "    1.77445263e-02   2.81692427e-02  -4.01458107e-02  -1.36897378e-02\n",
      "    1.21348053e-02  -7.46440608e-04   2.79204790e-02   1.64661128e-02\n",
      "   -2.65829754e-03  -5.59730120e-02  -5.93658537e-03  -3.25935073e-02\n",
      "   -4.97472472e-02  -2.09252741e-02  -7.10084685e-04  -4.37177764e-03\n",
      "    1.68588012e-02  -6.69977104e-04   3.04403733e-02   6.69327378e-03\n",
      "   -5.47360571e-04   1.79449730e-02   1.63237862e-02   1.68549269e-02\n",
      "   -3.16786133e-02  -5.55158406e-02  -3.85852344e-02   1.83067340e-02\n",
      "   -1.04869837e-02  -3.12737860e-02   4.50429395e-02   1.87315140e-03]]\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 1.36842811  0.07760302 -0.40128431 -0.1294892   0.42968893  1.97642028\n",
      "  -0.79717761 -0.93774587  0.52527785 -1.13988101  0.41472745  0.37880442\n",
      "   1.33286524 -0.97571713 -0.34202728  0.30019256 -0.27119648  1.12760341\n",
      "   0.74889898 -0.84463608 -2.02211189 -0.83246255  1.87073278  0.02654141\n",
      "  -0.73060018 -0.71392769  1.0609237  -0.24754216  0.54353243 -0.32486662\n",
      "   0.94848543  0.92289734  0.4835116  -2.19329095 -1.64967597 -0.36780179\n",
      "   0.78772271 -0.04879736  0.19357368  2.04880977  0.8811416   1.36697137\n",
      "  -0.63865894  0.0510393   0.47000012 -0.49715689  1.42867172 -0.68080032\n",
      "  -0.55952257 -0.95574015 -0.10957065 -0.44218981 -1.35100853  0.8052097\n",
      "  -0.28443822 -0.27579671  1.68750501 -2.00685287  0.22802021  0.57927525\n",
      "   0.21599796 -0.34283209  0.85602695 -0.03775672 -0.27466676  0.7226755\n",
      "  -1.52214038 -0.93989247  0.84824997  0.89630145 -1.09698844  1.57921243\n",
      "   0.78950626  0.17146885 -0.60674047  0.45701745 -0.11227421  1.64355445\n",
      "   0.21311599  1.08884263 -0.47637188  0.34326354 -0.99298018 -2.84219551\n",
      "   0.45582348  2.18855166 -1.35099339 -0.46447495 -1.20512128 -0.55659652\n",
      "  -2.06904936  0.20648496 -0.63608873 -0.20313543  1.48161376  1.8019619\n",
      "   0.65438914  0.78023952  0.36983752  1.50845027]]\n"
     ]
    }
   ],
   "source": [
    "np.shape(model.wv.syn0)\n",
    "test = np.vstack((np.zeros(100, dtype=np.float32), np.zeros(100, dtype=np.float32), model.wv.syn0))\n",
    "#print(type(emb_layer[0,0]))\n",
    "print(emb_layer[:3])\n",
    "print(test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cases = pool((Dataset.get_DS(stage='test', labelled='yes')).data[:-25])\n",
    "validation_cases = pool(Dataset.get_DS(stage='test', labelled='yes').data[-25:])\n",
    "test_cases = Dataset.get_DS(stage='train', labelled='yes')\n",
    "\n",
    "train_cases.process_for_s2s_testing()\n",
    "validation_cases.process_for_s2s_testing()\n",
    "test_cases.process_for_s2s_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_attention_seq2seq/rnn/embedding_wrapper/embedding:0\n",
      "embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel:0\n",
      "embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/embedding:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel:0\n",
      "embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = tf.nn.rnn_cell.LSTMCell(100)\n",
    "a, b = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq([tf.constant([0])], [tf.constant([0])], cell, 1, 1, 5)\n",
    "[print(v.name) for v in tf.trainable_variables() if \"embedding\" in v.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {}\n",
    "sets['train_set'], sets['train_labels'], _ = train_cases.get_s2s_sets(word_indices)\n",
    "sets['validation_set'], sets['validation_labels'], _ = validation_cases.get_s2s_sets(word_indices)\n",
    "sets['test_set'], sets['test_labels'], sets['test_words'] = test_cases.get_s2s_sets(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "\n",
    "from Iterator import RNNIterator\n",
    "\n",
    "\n",
    "class S2S_Model:\n",
    "\n",
    "    def __init__(self, enc_sym, dec_sym, emb_size, state_size, dropout=1.0, learn_rate=0.01, emb_layer=False):\n",
    "        self.enc_sym = enc_sym\n",
    "        self.dec_sym = dec_sym\n",
    "        self.emb_size = emb_size\n",
    "        self.state_size = state_size\n",
    "        self.dropout = dropout\n",
    "        self.learn_rate = learn_rate\n",
    "        self.emb_layer = emb_layer\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "\n",
    "    def reset_graph(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.reset_graph()\n",
    "\n",
    "        # Placeholders\n",
    "        x = tf.placeholder(tf.int32, shape=[None, 61])  # [batch_size, num_steps]\n",
    "        y = tf.placeholder(tf.int32, shape=[None, 61])\n",
    "        seqlen = tf.placeholder(tf.int32)\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        previous = tf.placeholder(tf.bool)\n",
    "        \n",
    "        x_in = tf.unstack(tf.transpose(x, perm=[1, 0]))\n",
    "        y_in = tf.unstack(tf.transpose(y, perm=[1, 0]))\n",
    "        \n",
    "        # RNN\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.state_size)\n",
    "        #cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "        s2s_outputs, final_state = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(encoder_inputs = x_in,\n",
    "                                                                                         decoder_inputs = y_in,\n",
    "                                                                                         cell = cell,\n",
    "                                                                                         num_encoder_symbols=self.enc_sym,\n",
    "                                                                                         num_decoder_symbols=self.dec_sym,\n",
    "                                                                                         embedding_size=self.emb_size,\n",
    "                                                                                         num_heads=1,\n",
    "                                                                                         output_projection=None,\n",
    "                                                                                         feed_previous=previous,\n",
    "                                                                                         initial_state_attention=False)\n",
    "        logits = tf.nn.softmax(s2s_outputs)\n",
    "        preds = tf.arg_max(logits, 1)\n",
    "        \n",
    "        # Softmax layer\n",
    "        #with tf.variable_scope('softmax'):\n",
    "        #    W = tf.Variable(tf.truncated_normal(shape=[self.state_size, self.num_classes], stddev=0.05))\n",
    "        #    b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]))\n",
    "        #logits = tf.matmul(last_rnn_output, W) + b\n",
    "        #preds = tf.nn.softmax(logits)\n",
    "        \n",
    "        #prediction = tf.argmax(preds, 1)\n",
    "\n",
    "        #loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        #train_step = tf.train.AdamOptimizer(self.learn_rate).minimize(loss)\n",
    "\n",
    "        self.graph = {'x': x,\n",
    "                      'y': y,\n",
    "                      'keep_prob': keep_prob,\n",
    "                      'previous': previous,\n",
    "                      's2s_outputs': s2s_outputs}\n",
    "\n",
    "    def train(self, sets, epochs=10, batch=50, report_percentage=1, show_progress=False, show_plot=False):\n",
    "        # Start a tf session and run the optimisation algorithm\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        trainer = RNNIterator(sets['train_set'], np.argmax(sets['train_labels'], 1), sets['train_lengths'])\n",
    "\n",
    "        train_truth = np.argmax(sets['train_labels'], 1)\n",
    "        validation_truth = np.argmax(sets['validation_labels'], 1)\n",
    "        test_truth = np.argmax(sets['test_labels'], 1)\n",
    "\n",
    "        train_feed = {self.graph['x']: sets['train_set'],\n",
    "                      self.graph['y']: np.argmax(sets['train_labels'], 1),\n",
    "                      self.graph['seqlen']: sets['train_lengths'],\n",
    "                      self.graph['keep_prob']: 1.0}\n",
    "        validation_feed = {self.graph['x']: sets['validation_set'],\n",
    "                           self.graph['y']: np.argmax(sets['validation_labels'], 1),\n",
    "                           self.graph['seqlen']: sets['validation_lengths'],\n",
    "                           self.graph['keep_prob']: 1.0}\n",
    "        test_feed = {self.graph['x']: sets['test_set'],\n",
    "                     self.graph['y']: np.argmax(sets['test_labels'], 1),\n",
    "                     self.graph['seqlen']: sets['test_lengths'],\n",
    "                     self.graph['keep_prob']: 1.0}\n",
    "\n",
    "        train_f1_score = []\n",
    "        validation_f1_score = []\n",
    "\n",
    "        mark = (epochs * (len(sets['train_set']) // batch) * report_percentage) // 100\n",
    "        check_point = []\n",
    "        N = 0\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        while trainer.epochs < epochs:\n",
    "            trd, trl, trle = trainer.next_batch(batch)\n",
    "            if N % mark == 0:\n",
    "                prediction = self.sess.run(self.graph['prediction'], feed_dict=train_feed)\n",
    "                train_f1_score.append(sk.metrics.f1_score(train_truth, prediction, pos_label=0))\n",
    "                prediction = self.sess.run(self.graph['prediction'], feed_dict=validation_feed)\n",
    "                validation_f1_score.append(sk.metrics.f1_score(validation_truth, prediction, pos_label=0))\n",
    "                check_point.append(N)\n",
    "                if show_progress: print(\"Progress: %d%%\" % (N * report_percentage // mark), end=\"\\r\")\n",
    "            feed = {self.graph['x']: trd, self.graph['y']: trl, self.graph['seqlen']: trle, self.graph['keep_prob']: self.dropout}\n",
    "            self.sess.run(self.graph['ts'], feed_dict=feed)\n",
    "            N += 1\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "        if show_plot:\n",
    "            np_check_point = np.array(check_point)\n",
    "            np_train_f1 = np.array(train_f1_score)\n",
    "            np_val_f1 = np.array(validation_f1_score)\n",
    "\n",
    "            plt.plot(np_check_point, np_train_f1, label=\"Train\")\n",
    "            plt.plot(np_check_point, np_val_f1, label=\"Validation\")\n",
    "            plt.plot(np_check_point, np.ones(len(np_check_point))*0.35, label=\"Baseline\")\n",
    "            plt.xlabel(\"Batches\")\n",
    "            plt.ylabel(\"F1-Score\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        test_f1_score = sk.metrics.f1_score(test_truth, self.sess.run(self.graph['prediction'], feed_dict=test_feed),\n",
    "                                            pos_label=0)\n",
    "        if show_progress:\n",
    "            print('FInal Values: Tr-F1: {:.4f}, Val-F1: {:.4f}'.format(train_f1_score[-1], validation_f1_score[-1]))\n",
    "            print(\"Test F1-Score: {:.4f}\\n\".format(test_f1_score))\n",
    "        return train_f1_score, validation_f1_score, test_f1_score\n",
    "\n",
    "    def predict(self, data, seqlen):\n",
    "        dummy = [1 for i in range(len(data))]\n",
    "        feed = {self.graph['x']: data, self.graph['y']: dummy, self.graph['seqlen']: seqlen, self.graph['keep_prob']: 1.0}\n",
    "        return self.sess.run(self.graph['prediction'], feed_dict=feed)\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S = S2S_Model(enc_sym=len(word_indices),\n",
    "                dec_sym=6,\n",
    "                emb_size=len(emb_layer[0]),\n",
    "                state_size=100,\n",
    "                learn_rate=0.001,\n",
    "                emb_layer=emb_layer)\n",
    "S2S.build_graph()\n",
    "#results_list = RNN.train(sets=sets, epochs=5, batch=50, report_percentage=1, show_progress=True, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument must be a dense tensor: [array([[-0.12008914,  0.21895055, -0.14263801,  0.03613903,  0.03949914,\n        -0.05319323]], dtype=float32), array([[-0.13080209,  0.22006416, -0.12917237,  0.05272328,  0.04248069,\n        -0.05863551]], dtype=float32), array([[-0.14260025,  0.22366118, -0.11931948,  0.0601925 ,  0.04219366,\n        -0.06657192]], dtype=float32), array([[-0.15230566,  0.22749868, -0.11189627,  0.06419074,  0.04103793,\n        -0.07333551]], dtype=float32), array([[-0.16017942,  0.23118873, -0.10617298,  0.06606901,  0.03940512,\n        -0.07893278]], dtype=float32), array([[-0.16656819,  0.2345047 , -0.10166955,  0.06662822,  0.03754642,\n        -0.0835712 ]], dtype=float32), array([[-0.17179909,  0.23735973, -0.09807751,  0.06634965,  0.03561754,\n        -0.08747087]], dtype=float32), array([[-0.17613699,  0.2397463 , -0.09518957,  0.0655349 ,  0.03371098,\n        -0.09080699]], dtype=float32), array([[-0.17977872,  0.24169622, -0.09286059,  0.06438202,  0.03188143,\n        -0.09370328]], dtype=float32), array([[-0.18286507,  0.24325873, -0.09098545,  0.0630274 ,  0.03016131,\n        -0.09624289]], dtype=float32), array([[-0.18549624,  0.24448836, -0.08948408,  0.06156821,  0.02856915,\n        -0.09848139]], dtype=float32), array([[-0.18774468,  0.24543984, -0.08829349,  0.06007423,  0.02711447,\n        -0.10045762]], dtype=float32), array([[-0.18966648,  0.24616326, -0.08736235,  0.05859563,  0.0258002 ,\n        -0.10220101]], dtype=float32), array([[-0.19130594,  0.24670403, -0.08664723,  0.05716722,  0.02462449,\n        -0.10373595]], dtype=float32), array([[-0.19270062,  0.24710147, -0.08611155,  0.05581263,  0.02358199,\n        -0.10508408]], dtype=float32), array([[-0.19388293,  0.2473886 , -0.08572364,  0.05454599,  0.02266486,\n        -0.10626519]], dtype=float32), array([[-0.19488159,  0.24759285, -0.08545622,  0.05337493,  0.02186362,\n        -0.10729748]], dtype=float32), array([[-0.19572212,  0.24773614, -0.08528598,  0.05230213,  0.02116787,\n        -0.10819796]], dtype=float32), array([[-0.1964274 ,  0.24783576, -0.0851929 ,  0.05132652,  0.02056696,\n        -0.10898226]], dtype=float32), array([[-0.19701748,  0.24790499, -0.08515996,  0.05044485,  0.02005026,\n        -0.10966463]], dtype=float32), array([[-0.19751023,  0.24795397, -0.08517298,  0.04965194,  0.01960754,\n        -0.11025763]], dtype=float32), array([[-0.19792071,  0.24798977, -0.08521997,  0.04894204,  0.0192293 ,\n        -0.11077265]], dtype=float32), array([[-0.19826251,  0.2480173 , -0.08529107,  0.04830886,  0.01890691,\n        -0.11121946]], dtype=float32), array([[-0.19854678,  0.2480403 , -0.0853785 ,  0.04774585,  0.01863252,\n        -0.11160698]], dtype=float32), array([[-0.19878319,  0.24806051, -0.0854756 ,  0.04724662,  0.0183991 ,\n        -0.1119427 ]], dtype=float32), array([[-0.19898012,  0.24807955, -0.08557767,  0.04680521,  0.01820068,\n        -0.11223327]], dtype=float32), array([[-0.1991442 ,  0.24809811, -0.08568066,  0.04641581,  0.01803194,\n        -0.11248462]], dtype=float32), array([[-0.19928108,  0.24811617, -0.08578178,  0.04607316,  0.01788841,\n        -0.11270176]], dtype=float32), array([[-0.19939536,  0.24813394, -0.08587889,  0.0457722 ,  0.01776621,\n        -0.11288903]], dtype=float32), array([[-0.19949119,  0.24815127, -0.0859707 ,  0.04550859,  0.0176621 ,\n        -0.11305037]], dtype=float32), array([[-0.19957145,  0.24816798, -0.08605614,  0.04527797,  0.01757319,\n        -0.11318906]], dtype=float32), array([[-0.19963916,  0.24818389, -0.08613481,  0.04507675,  0.01749731,\n        -0.11330824]], dtype=float32), array([[-0.19969612,  0.24819873, -0.08620644,  0.04490152,  0.01743241,\n        -0.11341041]], dtype=float32), array([[-0.19974428,  0.24821252, -0.08627127,  0.04474927,  0.01737692,\n        -0.11349782]], dtype=float32), array([[-0.19978513,  0.24822523, -0.08632942,  0.04461709,  0.0173293 ,\n        -0.11357236]], dtype=float32), array([[-0.19981985,  0.24823649, -0.08638122,  0.04450278,  0.01728859,\n        -0.11363591]], dtype=float32), array([[-0.21025296,  0.24511324, -0.07245694,  0.0351437 ,  0.0039578 ,\n        -0.11719982]], dtype=float32), array([[-0.21843852,  0.24211901, -0.06258185,  0.02653457, -0.00624958,\n        -0.1208025 ]], dtype=float32), array([[-0.22005922,  0.24224961, -0.06339194,  0.02650352, -0.00369999,\n        -0.12409136]], dtype=float32), array([[-0.19865932,  0.24282014, -0.07848755,  0.03515343,  0.01109896,\n        -0.11536399]], dtype=float32), array([[-0.18439879,  0.24362954, -0.08983374,  0.04038846,  0.0214971 ,\n        -0.11047263]], dtype=float32), array([[-0.19038637,  0.24482435, -0.09142546,  0.03593719,  0.01882517,\n        -0.11564241]], dtype=float32), array([[-0.19331051,  0.246167  , -0.09352892,  0.03354243,  0.01789873,\n        -0.11850031]], dtype=float32), array([[-0.19499902,  0.2467341 , -0.09437322,  0.03301938,  0.01740864,\n        -0.11956522]], dtype=float32), array([[-0.19591865,  0.24693841, -0.09452127,  0.0336125 ,  0.01726848,\n        -0.11958604]], dtype=float32), array([[-0.19638214,  0.24699782, -0.09429331,  0.0348088 ,  0.01736844,\n        -0.11904239]], dtype=float32), array([[-0.19659628,  0.24702299, -0.09387467,  0.03626434,  0.01760763,\n        -0.11824059]], dtype=float32), array([[-0.19669338,  0.24706212, -0.09336883,  0.03776075,  0.01790565,\n        -0.11736831]], dtype=float32), array([[-0.19675325,  0.24713054, -0.09283143,  0.03916883,  0.01820521,\n        -0.11653337]], dtype=float32), array([[-0.19681993,  0.24722561, -0.09229111,  0.0404199 ,  0.01846948,\n        -0.11579166]], dtype=float32), array([[-0.19691479,  0.24733886, -0.09176248,  0.04148519,  0.0186781 ,\n        -0.11516652]], dtype=float32), array([[-0.19704428,  0.24746025, -0.09125306,  0.0423614 ,  0.01882279,\n        -0.11466141]], dtype=float32), array([[-0.1972062 ,  0.24758086, -0.09076719,  0.04305948,  0.01890404,\n        -0.11426851]], dtype=float32), array([[-0.19739443,  0.24769399, -0.09030765,  0.04359858,  0.01892741,\n        -0.11397438]], dtype=float32), array([[-0.19760048,  0.24779539, -0.08987676,  0.04400101,  0.01890169,\n        -0.11376376]], dtype=float32), array([[-0.19781579,  0.24788295, -0.08947622,  0.04428933,  0.0188368 ,\n        -0.11362102]], dtype=float32), array([[-0.19803314,  0.24795605, -0.0891073 ,  0.0444851 ,  0.01874245,\n        -0.11353201]], dtype=float32), array([[-0.19824603,  0.24801569, -0.08877093,  0.04460758,  0.01862805,\n        -0.11348417]], dtype=float32), array([[-0.19844966,  0.24806294, -0.08846718,  0.04467339,  0.01850146,\n        -0.11346696]], dtype=float32), array([[-0.19864045,  0.2480997 , -0.0881957 ,  0.04469648,  0.01836935,\n        -0.11347181]], dtype=float32), array([[-0.19881622,  0.24812782, -0.08795535,  0.04468847,  0.01823699,\n        -0.1134918 ]], dtype=float32)] - got shape [61, 1, 6], but wanted [61].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-188-0b1e14a47ae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                         \u001b[0mS2S\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keep_prob'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                         S2S.graph['previous']:False})\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#logits = tf.nn.softmax(out)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)\u001b[0m\n\u001b[0;32m    198\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m           expected_shape=expected_shape)\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m           self._initial_value = ops.convert_to_tensor(\n\u001b[1;32m--> 289\u001b[1;33m               initial_value, name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[0;32m    290\u001b[0m           shape = (self._initial_value.get_shape()\n\u001b[0;32m    291\u001b[0m                    if validate_shape else tensor_shape.unknown_shape())\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m    674\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    111\u001b[0m                                          as_ref=False):\n\u001b[0;32m    112\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m--> 102\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    103\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    381\u001b[0m                          \"\"\" - got shape %s, but wanted %s.\"\"\" % (\n\u001b[0;32m    382\u001b[0m                              \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnparray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m                              _GetDenseDimensions(values)))\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;31m# python/numpy default float type is float64. We prefer float32 instead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Argument must be a dense tensor: [array([[-0.12008914,  0.21895055, -0.14263801,  0.03613903,  0.03949914,\n        -0.05319323]], dtype=float32), array([[-0.13080209,  0.22006416, -0.12917237,  0.05272328,  0.04248069,\n        -0.05863551]], dtype=float32), array([[-0.14260025,  0.22366118, -0.11931948,  0.0601925 ,  0.04219366,\n        -0.06657192]], dtype=float32), array([[-0.15230566,  0.22749868, -0.11189627,  0.06419074,  0.04103793,\n        -0.07333551]], dtype=float32), array([[-0.16017942,  0.23118873, -0.10617298,  0.06606901,  0.03940512,\n        -0.07893278]], dtype=float32), array([[-0.16656819,  0.2345047 , -0.10166955,  0.06662822,  0.03754642,\n        -0.0835712 ]], dtype=float32), array([[-0.17179909,  0.23735973, -0.09807751,  0.06634965,  0.03561754,\n        -0.08747087]], dtype=float32), array([[-0.17613699,  0.2397463 , -0.09518957,  0.0655349 ,  0.03371098,\n        -0.09080699]], dtype=float32), array([[-0.17977872,  0.24169622, -0.09286059,  0.06438202,  0.03188143,\n        -0.09370328]], dtype=float32), array([[-0.18286507,  0.24325873, -0.09098545,  0.0630274 ,  0.03016131,\n        -0.09624289]], dtype=float32), array([[-0.18549624,  0.24448836, -0.08948408,  0.06156821,  0.02856915,\n        -0.09848139]], dtype=float32), array([[-0.18774468,  0.24543984, -0.08829349,  0.06007423,  0.02711447,\n        -0.10045762]], dtype=float32), array([[-0.18966648,  0.24616326, -0.08736235,  0.05859563,  0.0258002 ,\n        -0.10220101]], dtype=float32), array([[-0.19130594,  0.24670403, -0.08664723,  0.05716722,  0.02462449,\n        -0.10373595]], dtype=float32), array([[-0.19270062,  0.24710147, -0.08611155,  0.05581263,  0.02358199,\n        -0.10508408]], dtype=float32), array([[-0.19388293,  0.2473886 , -0.08572364,  0.05454599,  0.02266486,\n        -0.10626519]], dtype=float32), array([[-0.19488159,  0.24759285, -0.08545622,  0.05337493,  0.02186362,\n        -0.10729748]], dtype=float32), array([[-0.19572212,  0.24773614, -0.08528598,  0.05230213,  0.02116787,\n        -0.10819796]], dtype=float32), array([[-0.1964274 ,  0.24783576, -0.0851929 ,  0.05132652,  0.02056696,\n        -0.10898226]], dtype=float32), array([[-0.19701748,  0.24790499, -0.08515996,  0.05044485,  0.02005026,\n        -0.10966463]], dtype=float32), array([[-0.19751023,  0.24795397, -0.08517298,  0.04965194,  0.01960754,\n        -0.11025763]], dtype=float32), array([[-0.19792071,  0.24798977, -0.08521997,  0.04894204,  0.0192293 ,\n        -0.11077265]], dtype=float32), array([[-0.19826251,  0.2480173 , -0.08529107,  0.04830886,  0.01890691,\n        -0.11121946]], dtype=float32), array([[-0.19854678,  0.2480403 , -0.0853785 ,  0.04774585,  0.01863252,\n        -0.11160698]], dtype=float32), array([[-0.19878319,  0.24806051, -0.0854756 ,  0.04724662,  0.0183991 ,\n        -0.1119427 ]], dtype=float32), array([[-0.19898012,  0.24807955, -0.08557767,  0.04680521,  0.01820068,\n        -0.11223327]], dtype=float32), array([[-0.1991442 ,  0.24809811, -0.08568066,  0.04641581,  0.01803194,\n        -0.11248462]], dtype=float32), array([[-0.19928108,  0.24811617, -0.08578178,  0.04607316,  0.01788841,\n        -0.11270176]], dtype=float32), array([[-0.19939536,  0.24813394, -0.08587889,  0.0457722 ,  0.01776621,\n        -0.11288903]], dtype=float32), array([[-0.19949119,  0.24815127, -0.0859707 ,  0.04550859,  0.0176621 ,\n        -0.11305037]], dtype=float32), array([[-0.19957145,  0.24816798, -0.08605614,  0.04527797,  0.01757319,\n        -0.11318906]], dtype=float32), array([[-0.19963916,  0.24818389, -0.08613481,  0.04507675,  0.01749731,\n        -0.11330824]], dtype=float32), array([[-0.19969612,  0.24819873, -0.08620644,  0.04490152,  0.01743241,\n        -0.11341041]], dtype=float32), array([[-0.19974428,  0.24821252, -0.08627127,  0.04474927,  0.01737692,\n        -0.11349782]], dtype=float32), array([[-0.19978513,  0.24822523, -0.08632942,  0.04461709,  0.0173293 ,\n        -0.11357236]], dtype=float32), array([[-0.19981985,  0.24823649, -0.08638122,  0.04450278,  0.01728859,\n        -0.11363591]], dtype=float32), array([[-0.21025296,  0.24511324, -0.07245694,  0.0351437 ,  0.0039578 ,\n        -0.11719982]], dtype=float32), array([[-0.21843852,  0.24211901, -0.06258185,  0.02653457, -0.00624958,\n        -0.1208025 ]], dtype=float32), array([[-0.22005922,  0.24224961, -0.06339194,  0.02650352, -0.00369999,\n        -0.12409136]], dtype=float32), array([[-0.19865932,  0.24282014, -0.07848755,  0.03515343,  0.01109896,\n        -0.11536399]], dtype=float32), array([[-0.18439879,  0.24362954, -0.08983374,  0.04038846,  0.0214971 ,\n        -0.11047263]], dtype=float32), array([[-0.19038637,  0.24482435, -0.09142546,  0.03593719,  0.01882517,\n        -0.11564241]], dtype=float32), array([[-0.19331051,  0.246167  , -0.09352892,  0.03354243,  0.01789873,\n        -0.11850031]], dtype=float32), array([[-0.19499902,  0.2467341 , -0.09437322,  0.03301938,  0.01740864,\n        -0.11956522]], dtype=float32), array([[-0.19591865,  0.24693841, -0.09452127,  0.0336125 ,  0.01726848,\n        -0.11958604]], dtype=float32), array([[-0.19638214,  0.24699782, -0.09429331,  0.0348088 ,  0.01736844,\n        -0.11904239]], dtype=float32), array([[-0.19659628,  0.24702299, -0.09387467,  0.03626434,  0.01760763,\n        -0.11824059]], dtype=float32), array([[-0.19669338,  0.24706212, -0.09336883,  0.03776075,  0.01790565,\n        -0.11736831]], dtype=float32), array([[-0.19675325,  0.24713054, -0.09283143,  0.03916883,  0.01820521,\n        -0.11653337]], dtype=float32), array([[-0.19681993,  0.24722561, -0.09229111,  0.0404199 ,  0.01846948,\n        -0.11579166]], dtype=float32), array([[-0.19691479,  0.24733886, -0.09176248,  0.04148519,  0.0186781 ,\n        -0.11516652]], dtype=float32), array([[-0.19704428,  0.24746025, -0.09125306,  0.0423614 ,  0.01882279,\n        -0.11466141]], dtype=float32), array([[-0.1972062 ,  0.24758086, -0.09076719,  0.04305948,  0.01890404,\n        -0.11426851]], dtype=float32), array([[-0.19739443,  0.24769399, -0.09030765,  0.04359858,  0.01892741,\n        -0.11397438]], dtype=float32), array([[-0.19760048,  0.24779539, -0.08987676,  0.04400101,  0.01890169,\n        -0.11376376]], dtype=float32), array([[-0.19781579,  0.24788295, -0.08947622,  0.04428933,  0.0188368 ,\n        -0.11362102]], dtype=float32), array([[-0.19803314,  0.24795605, -0.0891073 ,  0.0444851 ,  0.01874245,\n        -0.11353201]], dtype=float32), array([[-0.19824603,  0.24801569, -0.08877093,  0.04460758,  0.01862805,\n        -0.11348417]], dtype=float32), array([[-0.19844966,  0.24806294, -0.08846718,  0.04467339,  0.01850146,\n        -0.11346696]], dtype=float32), array([[-0.19864045,  0.2480997 , -0.0881957 ,  0.04469648,  0.01836935,\n        -0.11347181]], dtype=float32), array([[-0.19881622,  0.24812782, -0.08795535,  0.04468847,  0.01823699,\n        -0.1134918 ]], dtype=float32)] - got shape [61, 1, 6], but wanted [61]."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out = sess.run(S2S.graph['s2s_outputs'], feed_dict={S2S.graph['x']:[sets['test_set'][0]],\n",
    "                                                        S2S.graph['y']:[sets['test_labels'][0]],\n",
    "                                                        S2S.graph['keep_prob']:1.0,\n",
    "                                                        S2S.graph['previous']:False})\n",
    "    out = tf.Variable(out)\n",
    "    print(tf.shape(out))\n",
    "    #logits = tf.nn.softmax(out)\n",
    "    #print(logits)\n",
    "    #print(tf.arg_max(tf.nn.softmax(out, 1), 1).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34606     0 37927 37489 39417 39984 10168 29731 24182  4838 24987 10835\n",
      "  4838 23539 18428 16984 45872 11617 23539 15090 16984 45872 39417 20411\n",
      " 23539 27866 16984 45872  6671 23539 45840 16984 45872  6671 23539 34606\n",
      " 16984 45872 16360 39417 16984 23539 34061 16984 45872 39417 20411 23539\n",
      " 25081 16984 45872 39417 30196  7961 16329 41798  9748 34228 34518 20392\n",
      " 43350]\n",
      "[61]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-4bf254265c2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #a = tf.range(27)\n",
    "    #a = tf.reshape(a, [3,3,3])\n",
    "    a = tf.constant(sets['test_set'][0])\n",
    "    print(a.eval())\n",
    "    print(tf.shape(a).eval())\n",
    "    b = np.array(a)\n",
    "    print(b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 11\n"
     ]
    }
   ],
   "source": [
    "maxl = 0\n",
    "minl = float('inf')\n",
    "for case in train_cases.data:\n",
    "    for inp in case.test_text:\n",
    "        maxl = max(maxl, len(inp))\n",
    "        minl = min(minl, len(inp))\n",
    "\n",
    "for case in validation_cases.data:\n",
    "    for inp in case.test_text:\n",
    "        maxl = max(maxl, len(inp))\n",
    "        minl = min(minl, len(inp))\n",
    "        \n",
    "for case in test_cases.data:\n",
    "    for inp in case.test_text:\n",
    "        maxl = max(maxl, len(inp))\n",
    "        minl = min(minl, len(inp))\n",
    "\n",
    "print(maxl, minl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = RNN.predict(sets['test_set'], sets['test_lengths'])\n",
    "tru = np.argmax(sets['test_labels'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_perf = fn.token_perf(res, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline = [0 if word in target_dict['reasons'] else 1 for word in sets['test_words']]\n",
    "sk.metrics.f1_score(tru, baseline, pos_label=0, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn.category_words(sets['test_words'], res, tru, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn.colour_text(sets['test_words'], res, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testers = Dataset.get_DS(stage='train', labelled='yes')\n",
    "phrase_perf = fn.phrase_perf(target, RNN, testers, word_indices, side_words=[lw, rw], tfpn=True, show_phrases=True, case_info=True, rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_sizes = [100]\n",
    "emb_models = 1\n",
    "target_saturations = [0.05, 0.1, 0.2, 0.5, 0.7]\n",
    "layer_sizes = [50]\n",
    "dropouts = [1.0]\n",
    "learn_rates = [0.01]\n",
    "epochs = [100]\n",
    "NN_num = 5\n",
    "\n",
    "case_num = len(emb_sizes)*emb_models*len(layer_sizes)*len(target_saturations)*len(epochs)*len(dropouts)*len(learn_rates)*NN_num\n",
    "print(case_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_performance = 0\n",
    "n = 1\n",
    "\n",
    "for emb_size in emb_sizes:\n",
    "    print('Model Number: %d/%d' %(n, case_num))\n",
    "    for i in range(emb_models):\n",
    "        model = Word2Vec(sentences, min_count=1, size=emb_size)\n",
    "        for saturation in target_saturations:\n",
    "            sets = fn.get_traintest2 (labelled_cases, model)\n",
    "            fn.saturate_training_set(sets, model, target_dict['medications'], saturation)\n",
    "            for layer_size in layer_sizes:\n",
    "                for drop in dropouts:\n",
    "                    for rate in learn_rates:\n",
    "                        for epoch in epochs:\n",
    "                            for j in range(NN_num):\n",
    "                                print('Model Number: %d/%d' %(n, case_num))\n",
    "                                print('ES: %d EM: %d sat: %f, LS: %d, drop: %f, LR: %f, epochs: %d, NN: %d' \\\n",
    "                                       % (emb_size, i, saturation, layer_size, drop, rate, epoch, j))\n",
    "                                NN = FF_Model(input_size=emb_size, layers=[layer_size], dropout=drop, learn_rate=rate)\n",
    "                                NN.build_graph()\n",
    "                                NN.train(sets, epochs=epoch)\n",
    "                                res = NN.predict(sets['test_set'])\n",
    "                                tru = np.argmax(sets['test_labels'], 1)\n",
    "                                perf = sk.metrics.f1_score(tru, res, pos_label=0)\n",
    "                                if perf > max_performance:\n",
    "                                    max_performance = perf\n",
    "                                    NN.save_model('gold')\n",
    "                                    model.save('gold/GOLDEMB')\n",
    "                                NN.close()\n",
    "                                n += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
