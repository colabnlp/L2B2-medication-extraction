{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import Functions as fn\n",
    "import Iterator as it\n",
    "from DS import DS\n",
    "from Set import pool\n",
    "from FFModel import FF_Model\n",
    "from RNNModel import RNN_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Load Complete\n",
      "Raw Labels Load Complete\n"
     ]
    }
   ],
   "source": [
    "Dataset = pool()\n",
    "Dataset.load_texts('raw_texts')\n",
    "Dataset.load_labels('raw_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Load Complete\n"
     ]
    }
   ],
   "source": [
    "target_dict = fn.load_labels('final_meta/labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585 238 10 10\n"
     ]
    }
   ],
   "source": [
    "train_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[:-10])\n",
    "validation_set = pool(data=(Dataset.get_DS(stage='test', labelled='yes')).data[-10:])\n",
    "test_set = Dataset.get_DS(stage='train', labelled='yes')\n",
    "set_1 = Dataset.get_DS(stage='train', labelled='no')\n",
    "set_2 = Dataset.get_DS(stage='test', labelled='no')\n",
    "set_1.append(set_2.data)\n",
    "set_1.append(train_set.data)\n",
    "emb_set = set_1\n",
    "print(emb_set.size, train_set.size, validation_set.size, test_set.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Load Complete\n"
     ]
    }
   ],
   "source": [
    "#emb_set.process_for_embedding()\n",
    "#sentences = emb_set.get_sentences()\n",
    "#fn.write_sentences(sentences, 'final_meta/sentences')\n",
    "sentences = fn.load_sentences('final_meta/sentences')\n",
    "\n",
    "#model = Word2Vec(sentences, min_count=1, size=100)\n",
    "#model.save('final_meta/W2V')\n",
    "model = Word2Vec.load('final_meta/W2V')\n",
    "\n",
    "vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer and Index Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indices Load Complete\n",
      "Embedding Layer Load Complete\n"
     ]
    }
   ],
   "source": [
    "#word_indices, emb_layer = fn.get_index_and_emb_layer(model)\n",
    "#fn.write_word_indices(word_indices, 's2s/word_indices')\n",
    "#fn.write_emb_layer(emb_layer, 's2s/emb_layer')\n",
    "\n",
    "word_indices = fn.load_word_indices('s2s/word_indices')\n",
    "emb_layer = fn.load_emb_layer('s2s/emb_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.process_for_s2s_testing()\n",
    "validation_set.process_for_s2s_testing()\n",
    "test_set.process_for_s2s_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vancomycin', '<start>', 'her', 'graft', 'the', 'remainder', 'of', 'the', 'hospital', 'course', 'was', 'unremarkable', 'on', 'the', '<num>', 'of', 'july', ',', 'she', 'was', 'discharged', 'back', 'to', 'the', 'rose-walt', 'hospital', 'in', 'coln', 'discharge', 'medications', 'vancomycin', '<num>', 'mg', 'iv', 'q', 'd', ',', 'ofloxacin', '<num>', 'mg', 'po', 'bid', '(', 'both', 'antibiotics', 'to', 'continue', 'for', 'an', 'additional', 'two', 'week', 'course', ')', ',', 'coumadin', 'with', 'target']\n",
      "58\n",
      "[[1], [0], [6], [6], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [2], [2], [3], [4], [4], [0], [1], [2], [2], [3], [4], [0], [0], [1], [0], [0], [5], [5], [5], [5], [5], [5], [0], [0], [1], [0], [0]]\n",
      "58\n",
      "['<go>', 'vancomycin', '<num>', 'mg', 'iv', 'q', 'd', 'for', 'an', 'additional', 'two', 'week', 'course']\n",
      "13\n",
      "['vancomycin', '<num>', 'mg', 'iv', 'q', 'd', 'for', 'an', 'additional', 'two', 'week', 'course', '<eos>']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "entry = 19\n",
    "\n",
    "print(test_set.data[0].enc_inputs[entry])\n",
    "print(len(test_set.data[0].enc_inputs[entry]))\n",
    "print(test_set.data[0].enc_labels[entry])\n",
    "print(len(test_set.data[0].enc_labels[entry]))\n",
    "print(test_set.data[0].dec_inputs[entry])\n",
    "print(len(test_set.data[0].dec_inputs[entry]))\n",
    "print(test_set.data[0].dec_outputs[entry])\n",
    "print(len(test_set.data[0].dec_outputs[entry]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 34\n"
     ]
    }
   ],
   "source": [
    "max_enc_inp = 0\n",
    "max_dec_inp = 0\n",
    "for setin in [train_set, validation_set, test_set]:\n",
    "    for case in setin.data:\n",
    "        for enc_inp  in case.enc_inputs:\n",
    "            max_enc_inp = max(max_enc_inp, len(enc_inp))\n",
    "        for dec_inp  in case.dec_inputs:\n",
    "            max_dec_inp = max(max_dec_inp, len(dec_inp))\n",
    "print(max_enc_inp, max_dec_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {}\n",
    "sets['train'] = train_set.get_s2s_sets(word_indices, max_enc_inp, max_dec_inp)\n",
    "sets['validation'] = validation_set.get_s2s_sets(word_indices, max_enc_inp, max_dec_inp)\n",
    "sets['test'] = test_set.get_s2s_sets(word_indices, max_enc_inp, max_dec_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42508, 2, 26523, 23549, 20418, 12155, 36588, 25755, 41240, 45523, 39154, 48730, 45523, 4560, 38419, 5702, 16969, 33093, 4560, 14886, 5702, 16969, 20418, 41595, 4560, 15287, 5702, 16969, 19084, 4560, 3690, 5702, 16969, 19084, 4560, 42508, 5702, 16969, 14636, 20418, 5702, 4560, 47344, 5702, 16969, 20418, 41595, 4560, 26054, 5702, 16969, 20418, 43717, 34798, 32693, 35388, 13376, 12646, 7917, 45125, 1943, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "97\n",
      "[[1], [0], [2], [3], [4], [4], [0], [0], [2], [2], [0], [1], [0], [0], [1], [2], [2], [4], [0], [1], [2], [2], [4], [4], [0], [1], [2], [2], [4], [0], [1], [2], [2], [4], [0], [1], [2], [2], [3], [4], [4], [0], [1], [2], [2], [4], [4], [0], [1], [2], [2], [4], [4], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "97\n",
      "61\n",
      "[3, 42508, 5702, 16969, 14636, 20418, 5702, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "7\n",
      "[42508, 5702, 16969, 14636, 20418, 5702, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(sets['test'][0][0])\n",
    "print(len(sets['test'][0][0]))\n",
    "print(sets['test'][1][0])\n",
    "print(len(sets['test'][1][0]))\n",
    "print(sets['test'][2][0])\n",
    "print(sets['test'][3][0])\n",
    "print(sets['test'][4][0])\n",
    "print(sets['test'][5][0])\n",
    "print(sets['test'][6][0])\n",
    "print(len(sets['test'][6][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-97e04faea6a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luka\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             raise TypeError(\"Expected sequence or array-like, got %s\" %\n\u001b[1;32m--> 114\u001b[1;33m                             type(x))\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'int'>"
     ]
    }
   ],
   "source": [
    "pred = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 1, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "sk.metrics.f1_score(sets['test'][4][0], pred, labels=[1, 2, 3, 4, 5, 6], average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "\n",
    "from Iterator import S2SIterator\n",
    "\n",
    "\n",
    "class S2S_Model:\n",
    "\n",
    "    def __init__(self, \n",
    "                 batch=50,\n",
    "                 enc_vocab_size=100, \n",
    "                 dec_vocab_size=9, \n",
    "                 enc_emb_size=100, \n",
    "                 dec_emb_size=100, \n",
    "                 state_size=100, \n",
    "                 dropout=1.0,\n",
    "                 learn_rate=0.001,\n",
    "                 max_gradient_norm=5,\n",
    "                 enc_emb_layer=False):\n",
    "        self.batch = batch\n",
    "        self.enc_vocab_size = enc_vocab_size\n",
    "        self.dec_vocab_size = dec_vocab_size\n",
    "        self.enc_emb_size = enc_emb_size\n",
    "        self.dec_emb_size = dec_emb_size\n",
    "        self.state_size = state_size\n",
    "        self.dropout = dropout\n",
    "        self.learn_rate = learn_rate\n",
    "        self.max_gradient_norm=max_gradient_norm\n",
    "        self.enc_emb_layer = enc_emb_layer\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "\n",
    "    def reset_graph(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.reset_graph()\n",
    "\n",
    "        # Placeholders\n",
    "        enc_x = tf.placeholder(name ='encoder_inputs', dtype=tf.int32, shape=[self.batch, None])  # [batch_size, num_steps]\n",
    "        enc_labs = tf.placeholder(dtype=tf.float32, shape=[self.batch, None, 1])\n",
    "        dec_x = tf.placeholder(name ='decoder_inputs', dtype=tf.int32, shape=[self.batch, None])\n",
    "        y = tf.placeholder(name='target', dtype=tf.int32, shape=[self.batch, None])\n",
    "        target_weights = tf.placeholder(tf.float32, shape=[self.batch, None])\n",
    "        enc_seqlen = tf.placeholder(tf.int32, shape=[self.batch])\n",
    "        dec_seqlen = tf.placeholder(tf.int32, shape=[self.batch])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        testing = tf.placeholder(tf.bool)\n",
    "        \n",
    "        # Embeddings\n",
    "        if type(self.enc_emb_layer) != bool:\n",
    "            enc_embeddings = tf.get_variable('enc_embedding_matrix', [self.enc_vocab_size, self.enc_emb_size], dtype=tf.float32)\n",
    "        else:\n",
    "            enc_embeddings = tf.get_variable('enc_embedding_matrix', initializer=tf.constant(self.enc_emb_layer))\n",
    "        enc_inputs = tf.nn.embedding_lookup(enc_embeddings, enc_x)\n",
    "        dec_inputs = tf.nn.embedding_lookup(enc_embeddings, dec_x)\n",
    "        enc_inputs = tf.concat([enc_inputs, enc_labs], axis=-1)\n",
    "        \n",
    "        # Encoder\n",
    "        \n",
    "        # Singl Forward\n",
    "        #encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(self.state_size)\n",
    "        \n",
    "        #encoder_outputs, encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell, \n",
    "        #                                                   inputs=enc_inputs,\n",
    "        #                                                   sequence_length=enc_seqlen,\n",
    "        #                                                   dtype=tf.float32)\n",
    "        \n",
    "        # Bidirectional\n",
    "        forward_cell = tf.nn.rnn_cell.BasicLSTMCell(self.state_size)\n",
    "        backward_cell = tf.nn.rnn_cell.BasicLSTMCell(self.state_size)\n",
    "\n",
    "        bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(forward_cell,\n",
    "                                                                    backward_cell, \n",
    "                                                                    enc_inputs,\n",
    "                                                                    sequence_length=enc_seqlen,\n",
    "                                                                    dtype=tf.float32)\n",
    "        \n",
    "        # Conbining the output hidden states of ceells\n",
    "        encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "        encoder_state = tf.concat(encoder_state, axis=-1)       \n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(encoder_state[0], encoder_state[1])\n",
    "        \n",
    "        #Decoder\n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(2*self.state_size)\n",
    "\n",
    "        # Attention\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(2*self.state_size, \n",
    "                                                                encoder_outputs,\n",
    "                                                                memory_sequence_length=enc_seqlen)\n",
    "        \n",
    "        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,\n",
    "                                                           attention_mechanism,\n",
    "                                                           attention_layer_size=self.state_size)\n",
    "        \n",
    "        attn_zero = decoder_cell.zero_state(batch_size=self.batch, dtype=tf.float32)\n",
    "        encoder_state = attn_zero.clone(cell_state=encoder_state)\n",
    "        \n",
    "        # Helper\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(dec_inputs, dec_seqlen)\n",
    "        \n",
    "        projection_layer = tf.contrib.keras.layers.Dense(self.dec_vocab_size, use_bias=False)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, \n",
    "                                                  helper, \n",
    "                                                  encoder_state,\n",
    "                                                  output_layer=projection_layer)\n",
    "        \n",
    "        # Dynamic decoding\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "        \n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        train_loss = (tf.reduce_sum(crossent * target_weights) / self.batch)\n",
    "        wordnum = tf.to_float(tf.reduce_mean(dec_seqlen))\n",
    "        ent_per_word = train_loss * self.batch / wordnum\n",
    "        perplexity = tf.pow(tf.to_float(2), ent_per_word)\n",
    "        \n",
    "        # Calculate and clip gradients\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(train_loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "        \n",
    "        # Optimization\n",
    "        optimizer = tf.train.AdamOptimizer(self.learn_rate)\n",
    "        update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "        \n",
    "        # Test Helper\n",
    "        test_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(enc_embeddings,\n",
    "                                                               tf.fill([tf.shape(enc_x)[0]], 3),\n",
    "                                                               4)\n",
    "\n",
    "        # Test Decoder\n",
    "        test_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, \n",
    "                                                       test_helper, \n",
    "                                                       encoder_state,\n",
    "                                                       output_layer=projection_layer)\n",
    "        # Test Dynamic decoding\n",
    "        maximum_iterations = tf.round(tf.reduce_max(enc_seqlen))\n",
    "        test_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(test_decoder,\n",
    "                                                               maximum_iterations=maximum_iterations)\n",
    "        #prediction = test_outputs.sample_id\n",
    "        prediction = tf.argmax(test_outputs.rnn_output, axis=2)\n",
    "\n",
    "        self.graph = {'enc_x': enc_x,\n",
    "                      'enc_labs': enc_labs,\n",
    "                      'dec_x': dec_x,\n",
    "                      'y': y,\n",
    "                      'enc_seqlen': enc_seqlen,\n",
    "                      'dec_seqlen': dec_seqlen,\n",
    "                      'target_weights': target_weights,\n",
    "                      'keep_prob': keep_prob,\n",
    "                      'prediction': prediction,\n",
    "                      'perplexity': perplexity,\n",
    "                      'update_step': update_step\n",
    "                      }\n",
    "\n",
    "    def train(self, sets, epochs=10, report_percentage=1, show_progress=False, show_plot=False):\n",
    "        # Start a tf session and run the optimisation algorithm\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        trainer = S2SIterator(*(sets['train'][:-1]))\n",
    "\n",
    "        train_truth = [truth[:end] for (truth, end) in zip(sets['train'][5], sets['train'][4])]\n",
    "        validation_truth = [truth[:l] for (truth, l) in zip(sets['validation'][5], sets['validation'][4])]\n",
    "        test_truth = [truth[:l] for (truth, l) in zip(sets['test'][5], sets['test'][4])]\n",
    "\n",
    "        train_feed = {self.graph['enc_x']: sets['train'][0],\n",
    "                      self.graph['enc_labs']: sets['train'][1],\n",
    "                      self.graph['enc_seqlen']: sets['train'][2],\n",
    "                      self.graph['dec_x']: sets['train'][3],\n",
    "                      self.graph['dec_seqlen']: sets['train'][4],\n",
    "                      self.graph['y']: sets['train'][5],\n",
    "                      self.graph['target_weights']: sets['train'][6],\n",
    "                      self.graph['keep_prob']: 1.0}\n",
    "        validation_feed = {self.graph['enc_x']: sets['validation'][0],\n",
    "                           self.graph['enc_labs']: sets['validation'][1],\n",
    "                           self.graph['enc_seqlen']: sets['validation'][2],\n",
    "                           self.graph['dec_x']: sets['validation'][3],\n",
    "                           self.graph['dec_seqlen']: sets['validation'][4],\n",
    "                           self.graph['y']: sets['validation'][5],\n",
    "                           self.graph['target_weights']: sets['validation'][6],\n",
    "                           self.graph['keep_prob']: 1.0}\n",
    "        test_feed = {self.graph['enc_x']: sets['test'][0],\n",
    "                     self.graph['enc_labs']: sets['test'][1],\n",
    "                     self.graph['enc_seqlen']: sets['test'][2],\n",
    "                     self.graph['dec_x']: sets['test'][3],\n",
    "                     self.graph['dec_seqlen']: sets['test'][4],\n",
    "                     self.graph['y']: sets['test'][5],\n",
    "                     self.graph['target_weights']: sets['test'][6],\n",
    "                     self.graph['keep_prob']: 1.0}\n",
    "\n",
    "        train_f1_score = []\n",
    "        validation_f1_score = []\n",
    "\n",
    "        mark = (epochs * (len(sets['train'][0]) // self.batch) * report_percentage) // 100\n",
    "        check_point = []\n",
    "        N = 0\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        while trainer.epochs < epochs:\n",
    "            trex, trel, trexl, trdx, trdxl, trdy, trtw = trainer.next_batch(self.batch)\n",
    "            feed = {self.graph['enc_x']: trex,\n",
    "                    self.graph['enc_labs']: trel,\n",
    "                    self.graph['enc_seqlen']: trexl, \n",
    "                    self.graph['dec_x']: trdx, \n",
    "                    self.graph['dec_seqlen']: trdxl,\n",
    "                    self.graph['y']: trdy,\n",
    "                    self.graph['target_weights']: trtw,\n",
    "                    self.graph['keep_prob']: self.dropout}\n",
    "            if N % mark == 0:\n",
    "                #prediction = self.sess.run(self.graph['prediction'], feed_dict=train_feed)\n",
    "                #pred_cut = [pred[:end] for (pred, end) in zip(prediction, sets['train'][3])]\n",
    "                #f1_sum = 0\n",
    "                #for i in range(len(pred_cut)):\n",
    "                #    f1_sum += sk.metrics.f1_score(train_truth[i], pred_cut[i], labels=[1, 2, 3, 4, 5, 6], average='micro')       \n",
    "                #train_f1_score.append(f1_sum / len(pred_cut))\n",
    "                #prediction = self.sess.run(self.graph['prediction'], feed_dict=validation_feed)\n",
    "                #pred_cut = [pred[:end] for (pred, end) in zip(prediction, sets['validation'][3])]\n",
    "                #f1_sum = 0\n",
    "                #for i in range(len(pred_cut)):\n",
    "                #    f1_sum += sk.metrics.f1_score(validation_truth[i], pred_cut[i], labels=[1, 2, 3, 4, 5, 6], average='micro')       \n",
    "                #validation_f1_score.append(f1_sum / len(pred_cut))\n",
    "                #check_point.append(N)\n",
    "                perp = self.sess.run(self.graph['perplexity'], feed_dict=feed)\n",
    "                print('Perplexity: {}'.format(perp))\n",
    "                if show_progress: print(\"Progress: %d%%\" % (N * report_percentage // mark), end=\"\\r\")\n",
    "            self.sess.run(self.graph['update_step'], feed_dict=feed)\n",
    "            N += 1\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "        #test_prediction = self.sess.run(self.graph['prediction'], feed_dict=test_feed)\n",
    "        #pred_cut = [pred[:end] for (pred, end) in zip(prediction, sets['test'][3])]\n",
    "        #f1_sum = 0\n",
    "        #for i in range(len(pred_cut)):\n",
    "        #    f1_sum += sk.metrics.f1_score(test_truth[i], pred_cut[i], labels=[1, 2, 3, 4, 5, 6], average='micro')       \n",
    "        #test_f1_score = f1_sum / len(pred_cut)\n",
    "        \n",
    "        #if show_progress:\n",
    "        #    print('FInal Values: Tr-F1: {:.4f}, Val-F1: {:.4f}'.format(train_f1_score[-1], validation_f1_score[-1]))\n",
    "        #    print(\"Test F1-Score: {:.4f}\\n\".format(test_f1_score))\n",
    "        \n",
    "        #if show_plot:\n",
    "        #    np_check_point = np.array(check_point)\n",
    "        #    np_train_f1 = np.array(train_f1_score)\n",
    "        #    np_val_f1 = np.array(validation_f1_score)\n",
    "\n",
    "        #    plt.plot(np_check_point, np_train_f1, label=\"Train\")\n",
    "        #    plt.plot(np_check_point, np_val_f1, label=\"Validation\")\n",
    "        #    plt.plot(np_check_point, np.ones(len(np_check_point))*0.35, label=\"Baseline\")\n",
    "        #    plt.xlabel(\"Batches\")\n",
    "        #    plt.ylabel(\"F1-Score\")\n",
    "        #    plt.legend()\n",
    "        #    plt.show()\n",
    "        \n",
    "        #return train_f1_score, validation_f1_score, test_f1_score\n",
    "\n",
    "    def predict(self, data):\n",
    "        feed = {self.graph['enc_x']: data[0],\n",
    "                self.graph['enc_labs']: data[1],\n",
    "                self.graph['enc_seqlen']: data[2], \n",
    "                self.graph['dec_x']: data[3], \n",
    "                self.graph['dec_seqlen']: data[4],\n",
    "                self.graph['y']: data[5],\n",
    "                self.graph['target_weights']: data[6],\n",
    "                self.graph['keep_prob']: 1.0}\n",
    "        return self.sess.run(self.graph['prediction'], feed_dict=feed)\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Perplexity: inf\n",
      "Progress: 100%\r"
     ]
    }
   ],
   "source": [
    "S2S = S2S_Model(batch=50,\n",
    "                enc_vocab_size=len(word_indices), \n",
    "                dec_vocab_size=len(word_indices), \n",
    "                enc_emb_size=100, \n",
    "                dec_emb_size=100, \n",
    "                state_size=128, \n",
    "                dropout=1.0,\n",
    "                learn_rate=0.0001,\n",
    "                max_gradient_norm=5,\n",
    "                enc_emb_layer=emb_layer)\n",
    "S2S.build_graph()\n",
    "S2S.train(sets=sets, epochs=1, report_percentage=10, show_progress=True, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_vocab = []\n",
    "tru_words = sets['test'][7]\n",
    "for i in range(len(sets['test'][5])):\n",
    "    tru_vocab.append(sets['test'][5][i][:sets['test'][4][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42508, 5702, 16969, 14636, 20418, 5702, 4], [48730, 33341, 5702, 26523, 23549, 19084, 4], [27732, 5702, 16969, 14636, 19084, 4]]\n",
      "[['ofloxacin', '<num>', 'mg', 'po', 'q', '<num>', '<eos>'], ['insulin', 'nph', '<num>', 'units', 'subcu', 'bid', '<eos>'], ['colace', '<num>', 'mg', 'po', 'bid', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print(tru_vocab[:3])\n",
    "print(tru_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(sets['test'][0])):\n",
    "    dummy = [[], [], [], [], [], [], []]\n",
    "    for j in range(50):\n",
    "        for k in range(7):\n",
    "            dummy[k].append(sets['test'][k][i])\n",
    "    temp = S2S.predict(dummy)\n",
    "    res.append(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(7):\n",
    "    dummy = [[], [], [], [], [], []]\n",
    "    for k in range(6):\n",
    "        print('bla')\n",
    "        dummy[k].append(sets['test'][k][i*50:(i+1)*50])\n",
    "    temp = S2S.predict(dummy)\n",
    "    res.append(temp)\n",
    "dummy = [[], [], [], [], [], []]\n",
    "for i in range(5):\n",
    "    for k in range(6):\n",
    "        dummy[k].append(sets['test'][k][350:])\n",
    "temp = S2S.predict(dummy)\n",
    "res.append(temp[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64),\n",
       " array([4], dtype=int64)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_indices:\n",
    "    if word_indices[word] == 40251:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append(sets['test'][0][2:5])\n",
    "print(len(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_perf = fn.token_perf(res, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline = [0 if word in target_dict['reasons'] else 1 for word in sets['test_words']]\n",
    "sk.metrics.f1_score(tru, baseline, pos_label=0, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn.category_words(sets['test_words'], res, tru, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn.colour_text(sets['test_words'], res, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testers = Dataset.get_DS(stage='train', labelled='yes')\n",
    "phrase_perf = fn.phrase_perf(target, RNN, testers, word_indices, side_words=[lw, rw], tfpn=True, show_phrases=True, case_info=True, rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_sizes = [100]\n",
    "emb_models = 1\n",
    "target_saturations = [0.05, 0.1, 0.2, 0.5, 0.7]\n",
    "layer_sizes = [50]\n",
    "dropouts = [1.0]\n",
    "learn_rates = [0.01]\n",
    "epochs = [100]\n",
    "NN_num = 5\n",
    "\n",
    "case_num = len(emb_sizes)*emb_models*len(layer_sizes)*len(target_saturations)*len(epochs)*len(dropouts)*len(learn_rates)*NN_num\n",
    "print(case_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_performance = 0\n",
    "n = 1\n",
    "\n",
    "for emb_size in emb_sizes:\n",
    "    print('Model Number: %d/%d' %(n, case_num))\n",
    "    for i in range(emb_models):\n",
    "        model = Word2Vec(sentences, min_count=1, size=emb_size)\n",
    "        for saturation in target_saturations:\n",
    "            sets = fn.get_traintest2 (labelled_cases, model)\n",
    "            fn.saturate_training_set(sets, model, target_dict['medications'], saturation)\n",
    "            for layer_size in layer_sizes:\n",
    "                for drop in dropouts:\n",
    "                    for rate in learn_rates:\n",
    "                        for epoch in epochs:\n",
    "                            for j in range(NN_num):\n",
    "                                print('Model Number: %d/%d' %(n, case_num))\n",
    "                                print('ES: %d EM: %d sat: %f, LS: %d, drop: %f, LR: %f, epochs: %d, NN: %d' \\\n",
    "                                       % (emb_size, i, saturation, layer_size, drop, rate, epoch, j))\n",
    "                                NN = FF_Model(input_size=emb_size, layers=[layer_size], dropout=drop, learn_rate=rate)\n",
    "                                NN.build_graph()\n",
    "                                NN.train(sets, epochs=epoch)\n",
    "                                res = NN.predict(sets['test_set'])\n",
    "                                tru = np.argmax(sets['test_labels'], 1)\n",
    "                                perf = sk.metrics.f1_score(tru, res, pos_label=0)\n",
    "                                if perf > max_performance:\n",
    "                                    max_performance = perf\n",
    "                                    NN.save_model('gold')\n",
    "                                    model.save('gold/GOLDEMB')\n",
    "                                NN.close()\n",
    "                                n += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
