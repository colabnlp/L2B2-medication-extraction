{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import Functions as fn\n",
    "from DS import DS\n",
    "from Set import pool\n",
    "from Iterator import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dataset = fn.firstTimeLoad()\n",
    "#Dataset.writeTexts()\n",
    "#Dataset.writeLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dataset = pool()\n",
    "Dataset.loadTexts()\n",
    "Dataset.loadLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Texts:  4605\n",
      "Number of 2007 Smoking Challenge texts:  926\n",
      "Number of 2008 ObesityChallenge texts:  1237\n",
      "Number of 2009 Medication Challenge texts:  1249\n",
      "Number of 2010 Relations Challenge texts:  694\n",
      "Number of 2011 Coreference Challenge texts:  188\n",
      "Number of 2012 Temporal Relations Challenge texts:  311\n",
      "Number of Train Texts:  2859\n",
      "Number of Test Texts:  1746\n",
      "Number of Labeled Texts:  258\n",
      "Number of Initially Labeled Texts:  10\n",
      "Number of Competitor Labeled Texts Texts:  248\n"
     ]
    }
   ],
   "source": [
    "print('Number of Texts: ', Dataset.size)\n",
    "print('Number of 2007 Smoking Challenge texts: ', Dataset.numberOf(challenge='2007 Smoking Challenge'))\n",
    "print('Number of 2008 ObesityChallenge texts: ', Dataset.numberOf(challenge='2008 Obesity Challenge'))\n",
    "print('Number of 2009 Medication Challenge texts: ', Dataset.numberOf(challenge='2009 Medication Challenge'))\n",
    "print('Number of 2010 Relations Challenge texts: ', Dataset.numberOf(challenge='2010 Relations Challenge'))\n",
    "print('Number of 2011 Coreference Challenge texts: ', Dataset.numberOf(challenge='2011 Coreference Challenge'))\n",
    "print('Number of 2012 Temporal Relations Challenge texts: ', Dataset.numberOf(challenge='2012 Temporal Relations Challenge'))\n",
    "print('Number of Train Texts: ', Dataset.numberOf(stage='train'))\n",
    "print('Number of Test Texts: ', Dataset.numberOf(stage='test'))\n",
    "print('Number of Labeled Texts: ', Dataset.numberOf(labelled='yes'))\n",
    "print('Number of Initially Labeled Texts: ', Dataset.numberOf(labelled='yes', label_type='train'))\n",
    "print('Number of Competitor Labeled Texts Texts: ', Dataset.numberOf(labelled='yes', label_type='test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.processForEmbedding()\n",
    "sentences = Dataset.getSentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928\n",
      "119\n",
      "76\n",
      "79\n",
      "80\n",
      "482\n"
     ]
    }
   ],
   "source": [
    "doc = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = set(doc.read().split('\\n'))\n",
    "stopwords.update(set(['nm', 'ngl', 'slides', 'descreases']))\n",
    "\n",
    "medications = set()\n",
    "dosages = set()\n",
    "modes = set()\n",
    "frequencies = set()\n",
    "durations = set()\n",
    "reasons = set()\n",
    "\n",
    "labelled = Dataset.getDS(labelled='yes')\n",
    "\n",
    "for case in labelled.data:\n",
    "    for term in re.finditer(r'm=\"[a-z0-9 ]+\"', case.raw_labels):\n",
    "        temp = term.group()[3:-1].split()\n",
    "        [medications.add(re.sub(r'\\d+', '<num>', word)) for word in temp if word not in stopwords]\n",
    "    for term in re.finditer(r'do=\"[a-z0-9 ]+\"', case.raw_labels):\n",
    "        temp = term.group()[4:-1].split()\n",
    "        [dosages.add(re.sub(r'\\d+', '<num>', word)) for word in temp if word not in stopwords]\n",
    "    for term in re.finditer(r'mo=\"[a-z0-9 ]+\"', case.raw_labels):\n",
    "        temp = term.group()[4:-1].split()\n",
    "        [modes.add(re.sub(r'\\d+', '<num>', word)) for word in temp if word not in stopwords]\n",
    "    for term in re.finditer(r'f=\"[a-z0-9 ]+\"', case.raw_labels):\n",
    "        temp = term.group()[3:-1].split()\n",
    "        [frequencies.add(re.sub(r'\\d+', '<num>', word)) for word in temp if word not in stopwords]\n",
    "    for term in re.finditer(r'du=\"[a-z0-9 ]+\"', case.raw_labels):\n",
    "        temp = term.group()[4:-1].split()\n",
    "        [durations.add(re.sub(r'\\d+', '<num>', word)) for word in temp if word not in stopwords]\n",
    "    for term in re.finditer(r'r=\"[a-z0-9 ]+\"', case.raw_labels):\n",
    "        temp = term.group()[3:-1].split()\n",
    "        [reasons.add(re.sub(r'\\d+', '<num>', word)) for word in temp if word not in stopwords]\n",
    "\n",
    "print(len(medications))\n",
    "print(len(dosages))\n",
    "print(len(modes))\n",
    "print(len(frequencies))\n",
    "print(len(durations))\n",
    "print(len(reasons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_I2B2 = Word2Vec(sentences, min_count=1, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physician', 0.7557651996612549),\n",
       " ('office', 0.7325965166091919),\n",
       " ('private', 0.7110618948936462),\n",
       " ('primary', 0.6850539445877075),\n",
       " ('cardiologist', 0.6723614931106567),\n",
       " ('pcp', 0.6335077285766602),\n",
       " ('local', 0.6258509159088135),\n",
       " ('oncologist', 0.6254763007164001),\n",
       " ('neurologist', 0.610754132270813),\n",
       " ('vandeyacht', 0.6047073602676392)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_I2B2.most_similar(\"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"da5438ea-07ef-4930-9adf-30b7d9a9c21b\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"da5438ea-07ef-4930-9adf-30b7d9a9c21b\");\n",
       "      el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete window._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"da5438ea-07ef-4930-9adf-30b7d9a9c21b\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'da5438ea-07ef-4930-9adf-30b7d9a9c21b' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.6.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.6.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"da5438ea-07ef-4930-9adf-30b7d9a9c21b\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.6.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.6.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.6.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.6.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"da5438ea-07ef-4930-9adf-30b7d9a9c21b\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        words.append(word)\n",
    "\n",
    "cnt = Counter(words).most_common(1000)\n",
    "cnt = np.array(cnt)\n",
    "topwords = np.ndarray.tolist(cnt[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285\n",
      "1285\n",
      "1285\n"
     ]
    }
   ],
   "source": [
    "visualisation = []\n",
    "colormap = []\n",
    "\n",
    "#[(visualisation.append(word), colormap.append('red')) for word in medications]\n",
    "#[(visualisation.append(word), colormap.append('green')) for word in dosages]\n",
    "#[(visualisation.append(word), colormap.append('purple')) for word in modes]\n",
    "#[(visualisation.append(word), colormap.append('yellow')) for word in frequencies]\n",
    "#[(visualisation.append(word), colormap.append('orange')) for word in durations]\n",
    "[(visualisation.append(word), colormap.append('cyan')) for word in reasons]\n",
    "\n",
    "for word in topwords:\n",
    "    if not word in visualisation:\n",
    "        visualisation.append(word)\n",
    "        colormap.append('blue')\n",
    "\n",
    "\n",
    "# This assumes words_top_ted is a list of strings, the top 1000 words\n",
    "words_vec = model_I2B2[visualisation]\n",
    "\n",
    "print(len(visualisation))\n",
    "print(len(words_vec))\n",
    "print(len(colormap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "words_tsne = tsne.fit_transform(words_vec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"word2vec T-SNE for most common words\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=words_tsne[:,0],\n",
    "                                    x2=words_tsne[:,1],\n",
    "                                    names=visualisation,\n",
    "                                    coloring=colormap))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", color=\"coloring\", size=8, source=source)\n",
    "\n",
    "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "train_labels = []\n",
    "train_size = 2000\n",
    "vocab = set(model_I2B2.wv.vocab.keys())\n",
    "target = set(reasons)\n",
    "\n",
    "for i in range(train_size * 9 // 10):\n",
    "    word = random.sample(vocab, 1)\n",
    "    train_set.append(model_I2B2[word[0]])\n",
    "    vocab.discard(word[0])\n",
    "    if word[0] in target:\n",
    "        train_labels.append([1,0])\n",
    "        target.discard(word[0])\n",
    "    else:\n",
    "        train_labels.append([0,1])\n",
    "    if i % 10 == 0:\n",
    "        word = random.sample(target, 1)\n",
    "        train_set.append(model_I2B2[word[0]])\n",
    "        train_labels.append([1,0])\n",
    "        target.discard(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "928\n"
     ]
    }
   ],
   "source": [
    "print(len(target))\n",
    "print(len(medications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09545455,  0.90454545])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_labels).sum(0)/len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "test_labels = []\n",
    "test_size = 500\n",
    "\n",
    "for i in range(int(test_size * 0.5)):\n",
    "    word = random.sample(vocab, 1)\n",
    "    test_set.append(model_I2B2[word[0]])\n",
    "    vocab.discard(word[0])\n",
    "    if word[0] in medications:\n",
    "        test_labels.append([1,0])\n",
    "        target.discard(word[0])\n",
    "    else:\n",
    "        test_labels.append([0,1])\n",
    "        \n",
    "for i in range(int(test_size * 0.5)):\n",
    "    word = random.sample(target, 1)\n",
    "    test_set.append(model_I2B2[word[0]])\n",
    "    test_labels.append([1,0])\n",
    "    target.discard(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.512,  0.488])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_labels).sum(0)/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luka\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0\n",
      "Epoch: 0, Training Accuracy: 0.900000, Test Accuracy: 0.498000\n",
      "Epoch: 102, Training Accuracy: 0.980000, Test Accuracy: 0.826000\n",
      "Epoch: 205, Training Accuracy: 0.940000, Test Accuracy: 0.826000\n",
      "Epoch: 307, Training Accuracy: 0.980000, Test Accuracy: 0.818000\n",
      "Epoch: 410, Training Accuracy: 1.000000, Test Accuracy: 0.822000\n",
      "Epoch: 512, Training Accuracy: 0.960000, Test Accuracy: 0.822000\n",
      "Epoch: 615, Training Accuracy: 1.000000, Test Accuracy: 0.822000\n",
      "Epoch: 717, Training Accuracy: 0.980000, Test Accuracy: 0.814000\n",
      "Epoch: 820, Training Accuracy: 0.960000, Test Accuracy: 0.828000\n",
      "Epoch: 923, Training Accuracy: 1.000000, Test Accuracy: 0.834000\n"
     ]
    }
   ],
   "source": [
    "node_count_1 = 50\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "# Define the first layer here\n",
    "W = weight_variable([100, node_count_1])\n",
    "b = bias_variable([node_count_1])\n",
    "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "# Use dropout for this layer (should you wish)\n",
    "#keep_prob = tf.placeholder(tf.float32)\n",
    "#h_drop = tf.nn.dropout(h1, keep_prob)\n",
    "\n",
    "# Define the output layer here\n",
    "V = weight_variable([node_count_1, 2])\n",
    "c = bias_variable([2])\n",
    "y = tf.nn.softmax(tf.matmul(h, V) + c)\n",
    "\n",
    "# We'll use the cross entropy loss function \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "\n",
    "# And classification accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# And the Adam optimiser\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(cross_entropy)\n",
    "\n",
    "# Start a tf session and run the optimisation algorithm\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "training = Iterator(train_set, train_labels)\n",
    "train_accuracy = 0\n",
    "test_accuracy = 0\n",
    "N = 0\n",
    "\n",
    "while training.epochs < 1000:\n",
    "    trd, trl = training.next_batch(50)\n",
    "    if N % 4000 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x: trd, y_: trl})\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x: test_set, y_: test_labels})\n",
    "        print(\"Epoch: %d, Training Accuracy: %f, Test Accuracy: %f\" % (training.epochs, train_accuracy, test_accuracy))\n",
    "    sess.run(train_step, feed_dict={x: trd, y_: trl})\n",
    "    N += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
